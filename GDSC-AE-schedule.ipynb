{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import preprocessing\n",
    "#import scipy.io as sio\n",
    "\n",
    "from models import VAE,AEBase\n",
    "from models import DNN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import models\n",
    "import utils as ut\n",
    "import copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "epochs = 500 #200,500,1000  \n",
    "#dim_au_in = 20049\n",
    "dim_au_out = 512 #8, 16, 32, 64, 128, 256,512\n",
    "dim_dnn_in = dim_au_out\n",
    "dim_dnn_out=1\n",
    "select_drug = 'Gefitinib'\n",
    "na = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_r=pd.read_csv('data/GDSCexpression.csv',index_col=0)\n",
    "label_r=pd.read_csv('data/GDSClabel.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_r=label_r.fillna(na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n"
     ]
    }
   ],
   "source": [
    "hvg,adata = ut.highly_variable_genes(data_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Erlotinib</th>\n",
       "      <th>AICAR</th>\n",
       "      <th>Camptothecin</th>\n",
       "      <th>Vinblastine</th>\n",
       "      <th>Cisplatin</th>\n",
       "      <th>Cytarabine</th>\n",
       "      <th>Docetaxel</th>\n",
       "      <th>Methotrexate</th>\n",
       "      <th>ATRA</th>\n",
       "      <th>Gefitinib</th>\n",
       "      <th>...</th>\n",
       "      <th>CMK</th>\n",
       "      <th>Pyrimethamine</th>\n",
       "      <th>JW-7-52-1</th>\n",
       "      <th>A-443654</th>\n",
       "      <th>GW843682X</th>\n",
       "      <th>MS-275</th>\n",
       "      <th>Parthenolide</th>\n",
       "      <th>MG-132</th>\n",
       "      <th>KIN001-135</th>\n",
       "      <th>TGX221</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BxPC-3</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.159708</td>\n",
       "      <td>0.188393</td>\n",
       "      <td>0.436127</td>\n",
       "      <td>0.154274</td>\n",
       "      <td>0.113764</td>\n",
       "      <td>0.249987</td>\n",
       "      <td>0.015324</td>\n",
       "      <td>0.015324</td>\n",
       "      <td>0.020293</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KMOE-2</th>\n",
       "      <td>0.002108</td>\n",
       "      <td>0.339814</td>\n",
       "      <td>0.318206</td>\n",
       "      <td>0.281740</td>\n",
       "      <td>0.063808</td>\n",
       "      <td>0.140681</td>\n",
       "      <td>0.101530</td>\n",
       "      <td>0.191210</td>\n",
       "      <td>0.031229</td>\n",
       "      <td>0.007419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022662</td>\n",
       "      <td>0.025217</td>\n",
       "      <td>0.270791</td>\n",
       "      <td>0.095608</td>\n",
       "      <td>0.139914</td>\n",
       "      <td>0.171747</td>\n",
       "      <td>0.026670</td>\n",
       "      <td>0.013904</td>\n",
       "      <td>0.002116</td>\n",
       "      <td>0.000956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MFM-223</th>\n",
       "      <td>0.002148</td>\n",
       "      <td>0.334087</td>\n",
       "      <td>0.122671</td>\n",
       "      <td>0.361438</td>\n",
       "      <td>0.051983</td>\n",
       "      <td>0.186659</td>\n",
       "      <td>0.228797</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>0.023043</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038997</td>\n",
       "      <td>0.006167</td>\n",
       "      <td>0.002185</td>\n",
       "      <td>0.102592</td>\n",
       "      <td>0.020338</td>\n",
       "      <td>0.010874</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.035206</td>\n",
       "      <td>0.007051</td>\n",
       "      <td>0.000323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUGC-3</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.095099</td>\n",
       "      <td>0.416792</td>\n",
       "      <td>0.422631</td>\n",
       "      <td>0.266409</td>\n",
       "      <td>0.237811</td>\n",
       "      <td>0.234952</td>\n",
       "      <td>0.011558</td>\n",
       "      <td>0.028838</td>\n",
       "      <td>0.153002</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OC-314</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.012740</td>\n",
       "      <td>0.479915</td>\n",
       "      <td>0.390145</td>\n",
       "      <td>0.067738</td>\n",
       "      <td>0.125869</td>\n",
       "      <td>0.249668</td>\n",
       "      <td>0.110418</td>\n",
       "      <td>0.002214</td>\n",
       "      <td>0.020309</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KP-N-S19s</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BC-3</th>\n",
       "      <td>0.003515</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048227</td>\n",
       "      <td>0.146476</td>\n",
       "      <td>0.400087</td>\n",
       "      <td>0.205247</td>\n",
       "      <td>0.147090</td>\n",
       "      <td>0.298803</td>\n",
       "      <td>0.023796</td>\n",
       "      <td>0.017219</td>\n",
       "      <td>0.002065</td>\n",
       "      <td>0.069181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Panc 08.13</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.152818</td>\n",
       "      <td>0.196279</td>\n",
       "      <td>0.180653</td>\n",
       "      <td>0.090963</td>\n",
       "      <td>0.093197</td>\n",
       "      <td>0.097267</td>\n",
       "      <td>0.018269</td>\n",
       "      <td>0.010237</td>\n",
       "      <td>0.009405</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EKVX</th>\n",
       "      <td>0.177238</td>\n",
       "      <td>0.078395</td>\n",
       "      <td>0.078938</td>\n",
       "      <td>0.072213</td>\n",
       "      <td>0.007225</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>0.129184</td>\n",
       "      <td>0.095642</td>\n",
       "      <td>0.111598</td>\n",
       "      <td>0.126364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020472</td>\n",
       "      <td>0.044047</td>\n",
       "      <td>0.243635</td>\n",
       "      <td>0.079375</td>\n",
       "      <td>0.019859</td>\n",
       "      <td>0.177759</td>\n",
       "      <td>0.014843</td>\n",
       "      <td>0.114431</td>\n",
       "      <td>0.007216</td>\n",
       "      <td>0.063911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DMS-114</th>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.127677</td>\n",
       "      <td>0.168317</td>\n",
       "      <td>0.281940</td>\n",
       "      <td>0.092074</td>\n",
       "      <td>0.191972</td>\n",
       "      <td>0.185916</td>\n",
       "      <td>0.007135</td>\n",
       "      <td>0.009329</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.043920</td>\n",
       "      <td>0.285969</td>\n",
       "      <td>0.075195</td>\n",
       "      <td>0.174841</td>\n",
       "      <td>0.008383</td>\n",
       "      <td>0.002135</td>\n",
       "      <td>0.080451</td>\n",
       "      <td>0.002135</td>\n",
       "      <td>0.028733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>789 rows × 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Erlotinib     AICAR  Camptothecin  Vinblastine  Cisplatin  \\\n",
       "BxPC-3      -1.000000  0.159708      0.188393     0.436127   0.154274   \n",
       "KMOE-2       0.002108  0.339814      0.318206     0.281740   0.063808   \n",
       "MFM-223      0.002148  0.334087      0.122671     0.361438   0.051983   \n",
       "NUGC-3      -1.000000  0.095099      0.416792     0.422631   0.266409   \n",
       "OC-314      -1.000000  0.012740      0.479915     0.390145   0.067738   \n",
       "...               ...       ...           ...          ...        ...   \n",
       "KP-N-S19s   -1.000000 -1.000000     -1.000000    -1.000000  -1.000000   \n",
       "BC-3         0.003515 -1.000000     -1.000000    -1.000000  -1.000000   \n",
       "Panc 08.13  -1.000000  0.152818      0.196279     0.180653   0.090963   \n",
       "EKVX         0.177238  0.078395      0.078938     0.072213   0.007225   \n",
       "DMS-114      0.000322  0.127677      0.168317     0.281940   0.092074   \n",
       "\n",
       "            Cytarabine  Docetaxel  Methotrexate      ATRA  Gefitinib  ...  \\\n",
       "BxPC-3        0.113764   0.249987      0.015324  0.015324   0.020293  ...   \n",
       "KMOE-2        0.140681   0.101530      0.191210  0.031229   0.007419  ...   \n",
       "MFM-223       0.186659   0.228797      0.003639  0.023043   0.003639  ...   \n",
       "NUGC-3        0.237811   0.234952      0.011558  0.028838   0.153002  ...   \n",
       "OC-314        0.125869   0.249668      0.110418  0.002214   0.020309  ...   \n",
       "...                ...        ...           ...       ...        ...  ...   \n",
       "KP-N-S19s    -1.000000  -1.000000     -1.000000 -1.000000  -1.000000  ...   \n",
       "BC-3         -1.000000  -1.000000     -1.000000 -1.000000  -1.000000  ...   \n",
       "Panc 08.13    0.093197   0.097267      0.018269  0.010237   0.009405  ...   \n",
       "EKVX          0.003470   0.129184      0.095642  0.111598   0.126364  ...   \n",
       "DMS-114       0.191972   0.185916      0.007135  0.009329   0.020822  ...   \n",
       "\n",
       "                 CMK  Pyrimethamine  JW-7-52-1  A-443654  GW843682X    MS-275  \\\n",
       "BxPC-3     -1.000000      -1.000000  -1.000000 -1.000000  -1.000000 -1.000000   \n",
       "KMOE-2      0.022662       0.025217   0.270791  0.095608   0.139914  0.171747   \n",
       "MFM-223     0.038997       0.006167   0.002185  0.102592   0.020338  0.010874   \n",
       "NUGC-3     -1.000000      -1.000000  -1.000000 -1.000000  -1.000000 -1.000000   \n",
       "OC-314     -1.000000      -1.000000  -1.000000 -1.000000  -1.000000 -1.000000   \n",
       "...              ...            ...        ...       ...        ...       ...   \n",
       "KP-N-S19s  -1.000000      -1.000000  -1.000000 -1.000000  -1.000000 -1.000000   \n",
       "BC-3        0.048227       0.146476   0.400087  0.205247   0.147090  0.298803   \n",
       "Panc 08.13 -1.000000      -1.000000  -1.000000 -1.000000  -1.000000 -1.000000   \n",
       "EKVX        0.020472       0.044047   0.243635  0.079375   0.019859  0.177759   \n",
       "DMS-114     0.002137       0.043920   0.285969  0.075195   0.174841  0.008383   \n",
       "\n",
       "            Parthenolide    MG-132  KIN001-135    TGX221  \n",
       "BxPC-3         -1.000000 -1.000000   -1.000000 -1.000000  \n",
       "KMOE-2          0.026670  0.013904    0.002116  0.000956  \n",
       "MFM-223         0.000970  0.035206    0.007051  0.000323  \n",
       "NUGC-3         -1.000000 -1.000000   -1.000000 -1.000000  \n",
       "OC-314         -1.000000 -1.000000   -1.000000 -1.000000  \n",
       "...                  ...       ...         ...       ...  \n",
       "KP-N-S19s      -1.000000 -1.000000   -1.000000 -1.000000  \n",
       "BC-3            0.023796  0.017219    0.002065  0.069181  \n",
       "Panc 08.13     -1.000000 -1.000000   -1.000000 -1.000000  \n",
       "EKVX            0.014843  0.114431    0.007216  0.063911  \n",
       "DMS-114         0.002135  0.080451    0.002135  0.028733  \n",
       "\n",
       "[789 rows x 139 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_idx = label_r.loc[:,select_drug]!=na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_r.columns = adata.var_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your is gene-cell, mine is cell-gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_r.loc[selected_idx,hvg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = label_r.loc[selected_idx,select_drug]\n",
    "scaler = preprocessing.StandardScaler(with_mean=True, with_std=True)\n",
    "data = scaler.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999999\n",
      "-9.065550491767716e-18\n"
     ]
    }
   ],
   "source": [
    "print(np.std(data))\n",
    "print(np.mean(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(675, 3462)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(789, 139)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_r.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split test train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data, label, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(675, 3462)\n",
      "(675,)\n",
      "(432, 3462) (432,)\n",
      "(135, 3462) (135,)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(label.shape)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.375997920629374\n",
      "-7.905185436786982\n"
     ]
    }
   ],
   "source": [
    "print(X_train.max())\n",
    "print(X_train.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add all data to AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainTensor = torch.FloatTensor(X_train).to(device)\n",
    "X_validTensor = torch.FloatTensor(X_valid).to(device)\n",
    "X_testTensor = torch.FloatTensor(X_test).to(device)\n",
    "X_allTensor = torch.FloatTensor(data).to(device)\n",
    "\n",
    "Y_trainTensor = torch.FloatTensor(Y_train.values).to(device)\n",
    "Y_validTensor = torch.FloatTensor(Y_valid.values).to(device)\n",
    "\n",
    "# construct TensorDataset\n",
    "train_dataset = TensorDataset(X_trainTensor, X_trainTensor)\n",
    "valid_dataset = TensorDataset(X_validTensor, X_validTensor)\n",
    "test_dataset = TensorDataset(X_testTensor, X_testTensor)\n",
    "all_dataset = TensorDataset(X_allTensor, X_allTensor)\n",
    "\n",
    "X_trainDataLoader = DataLoader(dataset=train_dataset, batch_size=200, shuffle=True)\n",
    "X_validDataLoader = DataLoader(dataset=valid_dataset, batch_size=200, shuffle=True)\n",
    "X_allDataLoader = DataLoader(dataset=all_dataset, batch_size=200, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = X_trainDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_train = {'train':X_trainDataLoader,'val':X_validDataLoader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainDataLoader.dataset.tensors[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 432, 'val': 108}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{x: dataloaders_train[x].dataset.tensors[0].shape[0] for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AEBase(input_dim=data.shape[1],latent_dim=256,hidden_dims=[1024,512,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AEBase(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=3462, out_features=1024, bias=True)\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (bottleneck): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (decoder_input): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=3462, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = VAE(dim_au_in=data_r.shape[1],dim_au_out=128)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_function = nn.SmoothL1Loss()\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ae_model(net,data_loaders,optimizer,loss_function,n_epochs,scheduler):\n",
    "    \n",
    "    dataset_sizes = {x: data_loaders[x].dataset.tensors[0].shape[0] for x in ['train', 'val']}\n",
    "    loss_train = {}\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(net.state_dict())\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, n_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                #optimizer = scheduler(optimizer, epoch)\n",
    "                net.train()  # Set model to training mode\n",
    "            else:\n",
    "                net.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            # for data in data_loaders[phase]:\n",
    "            for batchidx, (x, _) in enumerate(data_loaders[phase]):\n",
    "\n",
    "                x.requires_grad_(True)\n",
    "                # encode and decode \n",
    "                output = model(x)\n",
    "                # compute loss\n",
    "                loss = loss_function(output, x)      \n",
    "\n",
    "                # zero the parameter (weight) gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    # update the weights\n",
    "                    optimizer.step()\n",
    "\n",
    "                # print loss statistics\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            # Schedular\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            \n",
    "            loss_train[epoch,phase] = epoch_loss \n",
    "            print('{} Loss: {:.8f}'.format(phase, epoch_loss))\n",
    "            \n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    # Select best model wts\n",
    "    torch.save(best_model_wts, 'saved/models/ae.pkl')\n",
    "    net.load_state_dict(best_model_wts)           \n",
    "    \n",
    "    return net, loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00239341\n",
      "val Loss: 0.00300079\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00238665\n",
      "val Loss: 0.00300216\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00238214\n",
      "val Loss: 0.00300305\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00240891\n",
      "val Loss: 0.00300558\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00239257\n",
      "val Loss: 0.00300388\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00236371\n",
      "val Loss: 0.00300354\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00239391\n",
      "val Loss: 0.00300440\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00237672\n",
      "val Loss: 0.00300528\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00238567\n",
      "val Loss: 0.00300404\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00237047\n",
      "val Loss: 0.00300453\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00234157\n",
      "val Loss: 0.00300414\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00235259\n",
      "val Loss: 0.00300296\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00240162\n",
      "val Loss: 0.00300569\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00235590\n",
      "val Loss: 0.00300443\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00235226\n",
      "val Loss: 0.00300200\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00234642\n",
      "val Loss: 0.00300084\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00237760\n",
      "val Loss: 0.00300306\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00238500\n",
      "val Loss: 0.00300289\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00236629\n",
      "val Loss: 0.00300250\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00235446\n",
      "val Loss: 0.00300008\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00235165\n",
      "val Loss: 0.00300095\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00238079\n",
      "val Loss: 0.00300256\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00237560\n",
      "val Loss: 0.00300162\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00236359\n",
      "val Loss: 0.00300214\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00238146\n",
      "val Loss: 0.00300368\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00237937\n",
      "val Loss: 0.00300376\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00240555\n",
      "val Loss: 0.00300491\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.00235526\n",
      "val Loss: 0.00300342\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.00237992\n",
      "val Loss: 0.00300353\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.00234920\n",
      "val Loss: 0.00300186\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.00236389\n",
      "val Loss: 0.00300068\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.00236187\n",
      "val Loss: 0.00300112\n",
      "Epoch 32/499\n",
      "----------\n",
      "train Loss: 0.00236597\n",
      "val Loss: 0.00300165\n",
      "Epoch 33/499\n",
      "----------\n",
      "train Loss: 0.00235047\n",
      "val Loss: 0.00300070\n",
      "Epoch 34/499\n",
      "----------\n",
      "train Loss: 0.00235511\n",
      "val Loss: 0.00300015\n",
      "Epoch 35/499\n",
      "----------\n",
      "train Loss: 0.00235301\n",
      "val Loss: 0.00300094\n",
      "Epoch 36/499\n",
      "----------\n",
      "train Loss: 0.00236433\n",
      "val Loss: 0.00300132\n",
      "Epoch 37/499\n",
      "----------\n",
      "train Loss: 0.00236486\n",
      "val Loss: 0.00300180\n",
      "Epoch 38/499\n",
      "----------\n",
      "train Loss: 0.00236456\n",
      "val Loss: 0.00300082\n",
      "Epoch 39/499\n",
      "----------\n",
      "train Loss: 0.00236432\n",
      "val Loss: 0.00300260\n",
      "Epoch 40/499\n",
      "----------\n",
      "train Loss: 0.00236110\n",
      "val Loss: 0.00300427\n",
      "Epoch 41/499\n",
      "----------\n",
      "train Loss: 0.00235654\n",
      "val Loss: 0.00300253\n",
      "Epoch 42/499\n",
      "----------\n",
      "train Loss: 0.00239081\n",
      "val Loss: 0.00300268\n",
      "Epoch 43/499\n",
      "----------\n",
      "train Loss: 0.00237291\n",
      "val Loss: 0.00300334\n",
      "Epoch 44/499\n",
      "----------\n",
      "train Loss: 0.00236190\n",
      "val Loss: 0.00300345\n",
      "Epoch 45/499\n",
      "----------\n",
      "train Loss: 0.00238742\n",
      "val Loss: 0.00300466\n",
      "Epoch 46/499\n",
      "----------\n",
      "train Loss: 0.00236525\n",
      "val Loss: 0.00300374\n",
      "Epoch 47/499\n",
      "----------\n",
      "train Loss: 0.00237458\n",
      "val Loss: 0.00300376\n",
      "Epoch 48/499\n",
      "----------\n",
      "train Loss: 0.00236847\n",
      "val Loss: 0.00300410\n",
      "Epoch 49/499\n",
      "----------\n",
      "train Loss: 0.00237946\n",
      "val Loss: 0.00300305\n",
      "Epoch 50/499\n",
      "----------\n",
      "train Loss: 0.00236333\n",
      "val Loss: 0.00300126\n",
      "Epoch 51/499\n",
      "----------\n",
      "train Loss: 0.00236393\n",
      "val Loss: 0.00300257\n",
      "Epoch 52/499\n",
      "----------\n",
      "train Loss: 0.00235684\n",
      "val Loss: 0.00300279\n",
      "Epoch 53/499\n",
      "----------\n",
      "train Loss: 0.00236500\n",
      "val Loss: 0.00300333\n",
      "Epoch 54/499\n",
      "----------\n",
      "train Loss: 0.00234294\n",
      "val Loss: 0.00300298\n",
      "Epoch 55/499\n",
      "----------\n",
      "train Loss: 0.00238006\n",
      "val Loss: 0.00300278\n",
      "Epoch 56/499\n",
      "----------\n",
      "train Loss: 0.00237760\n",
      "val Loss: 0.00300256\n",
      "Epoch 57/499\n",
      "----------\n",
      "train Loss: 0.00239788\n",
      "val Loss: 0.00300349\n",
      "Epoch 58/499\n",
      "----------\n",
      "train Loss: 0.00235735\n",
      "val Loss: 0.00300189\n",
      "Epoch 59/499\n",
      "----------\n",
      "train Loss: 0.00238522\n",
      "val Loss: 0.00300492\n",
      "Epoch 60/499\n",
      "----------\n",
      "train Loss: 0.00240547\n",
      "val Loss: 0.00300381\n",
      "Epoch 61/499\n",
      "----------\n",
      "train Loss: 0.00236354\n",
      "val Loss: 0.00300330\n",
      "Epoch 62/499\n",
      "----------\n",
      "train Loss: 0.00235585\n",
      "val Loss: 0.00300212\n",
      "Epoch 63/499\n",
      "----------\n",
      "train Loss: 0.00235861\n",
      "val Loss: 0.00300343\n",
      "Epoch 64/499\n",
      "----------\n",
      "train Loss: 0.00237792\n",
      "val Loss: 0.00300338\n",
      "Epoch 65/499\n",
      "----------\n",
      "train Loss: 0.00236708\n",
      "val Loss: 0.00300342\n",
      "Epoch 66/499\n",
      "----------\n",
      "train Loss: 0.00236339\n",
      "val Loss: 0.00300332\n",
      "Epoch 67/499\n",
      "----------\n",
      "train Loss: 0.00234981\n",
      "val Loss: 0.00300355\n",
      "Epoch 68/499\n",
      "----------\n",
      "train Loss: 0.00238950\n",
      "val Loss: 0.00300398\n",
      "Epoch 69/499\n",
      "----------\n",
      "train Loss: 0.00237554\n",
      "val Loss: 0.00300251\n",
      "Epoch 70/499\n",
      "----------\n",
      "train Loss: 0.00240659\n",
      "val Loss: 0.00300503\n",
      "Epoch 71/499\n",
      "----------\n",
      "train Loss: 0.00234962\n",
      "val Loss: 0.00300559\n",
      "Epoch 72/499\n",
      "----------\n",
      "train Loss: 0.00235734\n",
      "val Loss: 0.00300470\n",
      "Epoch 73/499\n",
      "----------\n",
      "train Loss: 0.00236717\n",
      "val Loss: 0.00300440\n",
      "Epoch 74/499\n",
      "----------\n",
      "train Loss: 0.00237038\n",
      "val Loss: 0.00300414\n",
      "Epoch 75/499\n",
      "----------\n",
      "train Loss: 0.00236537\n",
      "val Loss: 0.00300265\n",
      "Epoch 76/499\n",
      "----------\n",
      "train Loss: 0.00237138\n",
      "val Loss: 0.00300292\n",
      "Epoch 77/499\n",
      "----------\n",
      "train Loss: 0.00237871\n",
      "val Loss: 0.00300144\n",
      "Epoch 78/499\n",
      "----------\n",
      "train Loss: 0.00237670\n",
      "val Loss: 0.00300201\n",
      "Epoch 79/499\n",
      "----------\n",
      "train Loss: 0.00236080\n",
      "val Loss: 0.00300196\n",
      "Epoch 80/499\n",
      "----------\n",
      "train Loss: 0.00235931\n",
      "val Loss: 0.00300212\n",
      "Epoch 81/499\n",
      "----------\n",
      "train Loss: 0.00235627\n",
      "val Loss: 0.00300279\n",
      "Epoch 82/499\n",
      "----------\n",
      "train Loss: 0.00236507\n",
      "val Loss: 0.00300344\n",
      "Epoch 83/499\n",
      "----------\n",
      "train Loss: 0.00238564\n",
      "val Loss: 0.00300338\n",
      "Epoch 84/499\n",
      "----------\n",
      "train Loss: 0.00240448\n",
      "val Loss: 0.00300446\n",
      "Epoch 85/499\n",
      "----------\n",
      "train Loss: 0.00235789\n",
      "val Loss: 0.00300337\n",
      "Epoch 86/499\n",
      "----------\n",
      "train Loss: 0.00242519\n",
      "val Loss: 0.00300545\n",
      "Epoch 87/499\n",
      "----------\n",
      "train Loss: 0.00238189\n",
      "val Loss: 0.00300519\n",
      "Epoch 88/499\n",
      "----------\n",
      "train Loss: 0.00238514\n",
      "val Loss: 0.00300651\n",
      "Epoch 89/499\n",
      "----------\n",
      "train Loss: 0.00237163\n",
      "val Loss: 0.00300676\n",
      "Epoch 90/499\n",
      "----------\n",
      "train Loss: 0.00238687\n",
      "val Loss: 0.00300497\n",
      "Epoch 91/499\n",
      "----------\n",
      "train Loss: 0.00235130\n",
      "val Loss: 0.00300333\n",
      "Epoch 92/499\n",
      "----------\n",
      "train Loss: 0.00237442\n",
      "val Loss: 0.00300325\n",
      "Epoch 93/499\n",
      "----------\n",
      "train Loss: 0.00236034\n",
      "val Loss: 0.00300387\n",
      "Epoch 94/499\n",
      "----------\n",
      "train Loss: 0.00234431\n",
      "val Loss: 0.00300379\n",
      "Epoch 95/499\n",
      "----------\n",
      "train Loss: 0.00238168\n",
      "val Loss: 0.00300486\n",
      "Epoch 96/499\n",
      "----------\n",
      "train Loss: 0.00240210\n",
      "val Loss: 0.00300615\n",
      "Epoch 97/499\n",
      "----------\n",
      "train Loss: 0.00236039\n",
      "val Loss: 0.00300431\n",
      "Epoch 98/499\n",
      "----------\n",
      "train Loss: 0.00236028\n",
      "val Loss: 0.00300535\n",
      "Epoch 99/499\n",
      "----------\n",
      "train Loss: 0.00241958\n",
      "val Loss: 0.00300560\n",
      "Epoch 100/499\n",
      "----------\n",
      "train Loss: 0.00240964\n",
      "val Loss: 0.00300620\n",
      "Epoch 101/499\n",
      "----------\n",
      "train Loss: 0.00235821\n",
      "val Loss: 0.00300463\n",
      "Epoch 102/499\n",
      "----------\n",
      "train Loss: 0.00238874\n",
      "val Loss: 0.00300467\n",
      "Epoch 103/499\n",
      "----------\n",
      "train Loss: 0.00236738\n",
      "val Loss: 0.00300329\n",
      "Epoch 104/499\n",
      "----------\n",
      "train Loss: 0.00231981\n",
      "val Loss: 0.00300218\n",
      "Epoch 105/499\n",
      "----------\n",
      "train Loss: 0.00239616\n",
      "val Loss: 0.00300272\n",
      "Epoch 106/499\n",
      "----------\n",
      "train Loss: 0.00238481\n",
      "val Loss: 0.00300324\n",
      "Epoch 107/499\n",
      "----------\n",
      "train Loss: 0.00233987\n",
      "val Loss: 0.00300295\n",
      "Epoch 108/499\n",
      "----------\n",
      "train Loss: 0.00235691\n",
      "val Loss: 0.00300318\n",
      "Epoch 109/499\n",
      "----------\n",
      "train Loss: 0.00236366\n",
      "val Loss: 0.00300344\n",
      "Epoch 110/499\n",
      "----------\n",
      "train Loss: 0.00233898\n",
      "val Loss: 0.00300125\n",
      "Epoch 111/499\n",
      "----------\n",
      "train Loss: 0.00235614\n",
      "val Loss: 0.00300166\n",
      "Epoch 112/499\n",
      "----------\n",
      "train Loss: 0.00238862\n",
      "val Loss: 0.00300382\n",
      "Epoch 113/499\n",
      "----------\n",
      "train Loss: 0.00236017\n",
      "val Loss: 0.00300379\n",
      "Epoch 114/499\n",
      "----------\n",
      "train Loss: 0.00241264\n",
      "val Loss: 0.00300580\n",
      "Epoch 115/499\n",
      "----------\n",
      "train Loss: 0.00234222\n",
      "val Loss: 0.00300284\n",
      "Epoch 116/499\n",
      "----------\n",
      "train Loss: 0.00234901\n",
      "val Loss: 0.00300319\n",
      "Epoch 117/499\n",
      "----------\n",
      "train Loss: 0.00235504\n",
      "val Loss: 0.00300244\n",
      "Epoch 118/499\n",
      "----------\n",
      "train Loss: 0.00237737\n",
      "val Loss: 0.00300412\n",
      "Epoch 119/499\n",
      "----------\n",
      "train Loss: 0.00240604\n",
      "val Loss: 0.00300495\n",
      "Epoch 120/499\n",
      "----------\n",
      "train Loss: 0.00235732\n",
      "val Loss: 0.00300439\n",
      "Epoch 121/499\n",
      "----------\n",
      "train Loss: 0.00236332\n",
      "val Loss: 0.00300543\n",
      "Epoch 122/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00237733\n",
      "val Loss: 0.00300417\n",
      "Epoch 123/499\n",
      "----------\n",
      "train Loss: 0.00236094\n",
      "val Loss: 0.00300220\n",
      "Epoch 124/499\n",
      "----------\n",
      "train Loss: 0.00238088\n",
      "val Loss: 0.00300221\n",
      "Epoch 125/499\n",
      "----------\n",
      "train Loss: 0.00238174\n",
      "val Loss: 0.00300346\n",
      "Epoch 126/499\n",
      "----------\n",
      "train Loss: 0.00234606\n",
      "val Loss: 0.00300180\n",
      "Epoch 127/499\n",
      "----------\n",
      "train Loss: 0.00240663\n",
      "val Loss: 0.00300271\n",
      "Epoch 128/499\n",
      "----------\n",
      "train Loss: 0.00233637\n",
      "val Loss: 0.00300117\n",
      "Epoch 129/499\n",
      "----------\n",
      "train Loss: 0.00236218\n",
      "val Loss: 0.00300069\n",
      "Epoch 130/499\n",
      "----------\n",
      "train Loss: 0.00240564\n",
      "val Loss: 0.00300253\n",
      "Epoch 131/499\n",
      "----------\n",
      "train Loss: 0.00237236\n",
      "val Loss: 0.00300365\n",
      "Epoch 132/499\n",
      "----------\n",
      "train Loss: 0.00238450\n",
      "val Loss: 0.00300383\n",
      "Epoch 133/499\n",
      "----------\n",
      "train Loss: 0.00237330\n",
      "val Loss: 0.00300320\n",
      "Epoch 134/499\n",
      "----------\n",
      "train Loss: 0.00236488\n",
      "val Loss: 0.00300214\n",
      "Epoch 135/499\n",
      "----------\n",
      "train Loss: 0.00237539\n",
      "val Loss: 0.00300252\n",
      "Epoch 136/499\n",
      "----------\n",
      "train Loss: 0.00233298\n",
      "val Loss: 0.00300183\n",
      "Epoch 137/499\n",
      "----------\n",
      "train Loss: 0.00237807\n",
      "val Loss: 0.00300200\n",
      "Epoch 138/499\n",
      "----------\n",
      "train Loss: 0.00238847\n",
      "val Loss: 0.00300161\n",
      "Epoch 139/499\n",
      "----------\n",
      "train Loss: 0.00240527\n",
      "val Loss: 0.00300400\n",
      "Epoch 140/499\n",
      "----------\n",
      "train Loss: 0.00233509\n",
      "val Loss: 0.00300427\n",
      "Epoch 141/499\n",
      "----------\n",
      "train Loss: 0.00234594\n",
      "val Loss: 0.00300343\n",
      "Epoch 142/499\n",
      "----------\n",
      "train Loss: 0.00237666\n",
      "val Loss: 0.00300279\n",
      "Epoch 143/499\n",
      "----------\n",
      "train Loss: 0.00237498\n",
      "val Loss: 0.00300509\n",
      "Epoch 144/499\n",
      "----------\n",
      "train Loss: 0.00240109\n",
      "val Loss: 0.00300517\n",
      "Epoch 145/499\n",
      "----------\n",
      "train Loss: 0.00234633\n",
      "val Loss: 0.00300301\n",
      "Epoch 146/499\n",
      "----------\n",
      "train Loss: 0.00241609\n",
      "val Loss: 0.00300476\n",
      "Epoch 147/499\n",
      "----------\n",
      "train Loss: 0.00235565\n",
      "val Loss: 0.00300362\n",
      "Epoch 148/499\n",
      "----------\n",
      "train Loss: 0.00237749\n",
      "val Loss: 0.00300504\n",
      "Epoch 149/499\n",
      "----------\n",
      "train Loss: 0.00236465\n",
      "val Loss: 0.00300525\n",
      "Epoch 150/499\n",
      "----------\n",
      "train Loss: 0.00240187\n",
      "val Loss: 0.00300562\n",
      "Epoch 151/499\n",
      "----------\n",
      "train Loss: 0.00238455\n",
      "val Loss: 0.00300575\n",
      "Epoch 152/499\n",
      "----------\n",
      "train Loss: 0.00239442\n",
      "val Loss: 0.00300598\n",
      "Epoch 153/499\n",
      "----------\n",
      "train Loss: 0.00243707\n",
      "val Loss: 0.00300743\n",
      "Epoch 154/499\n",
      "----------\n",
      "train Loss: 0.00235490\n",
      "val Loss: 0.00300677\n",
      "Epoch 155/499\n",
      "----------\n",
      "train Loss: 0.00234990\n",
      "val Loss: 0.00300758\n",
      "Epoch 156/499\n",
      "----------\n",
      "train Loss: 0.00238525\n",
      "val Loss: 0.00300516\n",
      "Epoch 157/499\n",
      "----------\n",
      "train Loss: 0.00241589\n",
      "val Loss: 0.00300529\n",
      "Epoch 158/499\n",
      "----------\n",
      "train Loss: 0.00238751\n",
      "val Loss: 0.00300486\n",
      "Epoch 159/499\n",
      "----------\n",
      "train Loss: 0.00236836\n",
      "val Loss: 0.00300439\n",
      "Epoch 160/499\n",
      "----------\n",
      "train Loss: 0.00237062\n",
      "val Loss: 0.00300326\n",
      "Epoch 161/499\n",
      "----------\n",
      "train Loss: 0.00236565\n",
      "val Loss: 0.00300246\n",
      "Epoch 162/499\n",
      "----------\n",
      "train Loss: 0.00235958\n",
      "val Loss: 0.00300232\n",
      "Epoch 163/499\n",
      "----------\n",
      "train Loss: 0.00235001\n",
      "val Loss: 0.00300316\n",
      "Epoch 164/499\n",
      "----------\n",
      "train Loss: 0.00237447\n",
      "val Loss: 0.00300424\n",
      "Epoch 165/499\n",
      "----------\n",
      "train Loss: 0.00237592\n",
      "val Loss: 0.00300295\n",
      "Epoch 166/499\n",
      "----------\n",
      "train Loss: 0.00238212\n",
      "val Loss: 0.00300395\n",
      "Epoch 167/499\n",
      "----------\n",
      "train Loss: 0.00235687\n",
      "val Loss: 0.00300236\n",
      "Epoch 168/499\n",
      "----------\n",
      "train Loss: 0.00234379\n",
      "val Loss: 0.00299976\n",
      "Epoch 169/499\n",
      "----------\n",
      "train Loss: 0.00237365\n",
      "val Loss: 0.00299956\n",
      "Epoch 170/499\n",
      "----------\n",
      "train Loss: 0.00237066\n",
      "val Loss: 0.00300275\n",
      "Epoch 171/499\n",
      "----------\n",
      "train Loss: 0.00236589\n",
      "val Loss: 0.00300102\n",
      "Epoch 172/499\n",
      "----------\n",
      "train Loss: 0.00238654\n",
      "val Loss: 0.00300094\n",
      "Epoch 173/499\n",
      "----------\n",
      "train Loss: 0.00234801\n",
      "val Loss: 0.00300103\n",
      "Epoch 174/499\n",
      "----------\n",
      "train Loss: 0.00237968\n",
      "val Loss: 0.00300106\n",
      "Epoch 175/499\n",
      "----------\n",
      "train Loss: 0.00235643\n",
      "val Loss: 0.00299998\n",
      "Epoch 176/499\n",
      "----------\n",
      "train Loss: 0.00236682\n",
      "val Loss: 0.00300059\n",
      "Epoch 177/499\n",
      "----------\n",
      "train Loss: 0.00235892\n",
      "val Loss: 0.00300098\n",
      "Epoch 178/499\n",
      "----------\n",
      "train Loss: 0.00238417\n",
      "val Loss: 0.00300274\n",
      "Epoch 179/499\n",
      "----------\n",
      "train Loss: 0.00237622\n",
      "val Loss: 0.00300404\n",
      "Epoch 180/499\n",
      "----------\n",
      "train Loss: 0.00236660\n",
      "val Loss: 0.00300520\n",
      "Epoch 181/499\n",
      "----------\n",
      "train Loss: 0.00236316\n",
      "val Loss: 0.00300359\n",
      "Epoch 182/499\n",
      "----------\n",
      "train Loss: 0.00235670\n",
      "val Loss: 0.00300301\n",
      "Epoch 183/499\n",
      "----------\n",
      "train Loss: 0.00234896\n",
      "val Loss: 0.00300279\n",
      "Epoch 184/499\n",
      "----------\n",
      "train Loss: 0.00236758\n",
      "val Loss: 0.00300252\n",
      "Epoch 185/499\n",
      "----------\n",
      "train Loss: 0.00236699\n",
      "val Loss: 0.00300247\n",
      "Epoch 186/499\n",
      "----------\n",
      "train Loss: 0.00236416\n",
      "val Loss: 0.00300119\n",
      "Epoch 187/499\n",
      "----------\n",
      "train Loss: 0.00237329\n",
      "val Loss: 0.00300154\n",
      "Epoch 188/499\n",
      "----------\n",
      "train Loss: 0.00236220\n",
      "val Loss: 0.00300093\n",
      "Epoch 189/499\n",
      "----------\n",
      "train Loss: 0.00236878\n",
      "val Loss: 0.00300303\n",
      "Epoch 190/499\n",
      "----------\n",
      "train Loss: 0.00237145\n",
      "val Loss: 0.00300380\n",
      "Epoch 191/499\n",
      "----------\n",
      "train Loss: 0.00233757\n",
      "val Loss: 0.00300216\n",
      "Epoch 192/499\n",
      "----------\n",
      "train Loss: 0.00238275\n",
      "val Loss: 0.00300206\n",
      "Epoch 193/499\n",
      "----------\n",
      "train Loss: 0.00242887\n",
      "val Loss: 0.00300353\n",
      "Epoch 194/499\n",
      "----------\n",
      "train Loss: 0.00236010\n",
      "val Loss: 0.00300239\n",
      "Epoch 195/499\n",
      "----------\n",
      "train Loss: 0.00236714\n",
      "val Loss: 0.00300352\n",
      "Epoch 196/499\n",
      "----------\n",
      "train Loss: 0.00236295\n",
      "val Loss: 0.00300323\n",
      "Epoch 197/499\n",
      "----------\n",
      "train Loss: 0.00239401\n",
      "val Loss: 0.00300325\n",
      "Epoch 198/499\n",
      "----------\n",
      "train Loss: 0.00241330\n",
      "val Loss: 0.00300625\n",
      "Epoch 199/499\n",
      "----------\n",
      "train Loss: 0.00237547\n",
      "val Loss: 0.00300560\n",
      "Epoch 200/499\n",
      "----------\n",
      "train Loss: 0.00238811\n",
      "val Loss: 0.00300620\n",
      "Epoch 201/499\n",
      "----------\n",
      "train Loss: 0.00239908\n",
      "val Loss: 0.00300699\n",
      "Epoch 202/499\n",
      "----------\n",
      "train Loss: 0.00236386\n",
      "val Loss: 0.00300424\n",
      "Epoch 203/499\n",
      "----------\n",
      "train Loss: 0.00235918\n",
      "val Loss: 0.00300431\n",
      "Epoch 204/499\n",
      "----------\n",
      "train Loss: 0.00235217\n",
      "val Loss: 0.00300317\n",
      "Epoch 205/499\n",
      "----------\n",
      "train Loss: 0.00240330\n",
      "val Loss: 0.00300375\n",
      "Epoch 206/499\n",
      "----------\n",
      "train Loss: 0.00238578\n",
      "val Loss: 0.00300195\n",
      "Epoch 207/499\n",
      "----------\n",
      "train Loss: 0.00237018\n",
      "val Loss: 0.00300206\n",
      "Epoch 208/499\n",
      "----------\n",
      "train Loss: 0.00238026\n",
      "val Loss: 0.00300255\n",
      "Epoch 209/499\n",
      "----------\n",
      "train Loss: 0.00235100\n",
      "val Loss: 0.00300171\n",
      "Epoch 210/499\n",
      "----------\n",
      "train Loss: 0.00235092\n",
      "val Loss: 0.00300243\n",
      "Epoch 211/499\n",
      "----------\n",
      "train Loss: 0.00238586\n",
      "val Loss: 0.00300226\n",
      "Epoch 212/499\n",
      "----------\n",
      "train Loss: 0.00237720\n",
      "val Loss: 0.00300115\n",
      "Epoch 213/499\n",
      "----------\n",
      "train Loss: 0.00239507\n",
      "val Loss: 0.00300273\n",
      "Epoch 214/499\n",
      "----------\n",
      "train Loss: 0.00239059\n",
      "val Loss: 0.00300347\n",
      "Epoch 215/499\n",
      "----------\n",
      "train Loss: 0.00235843\n",
      "val Loss: 0.00300347\n",
      "Epoch 216/499\n",
      "----------\n",
      "train Loss: 0.00239003\n",
      "val Loss: 0.00300342\n",
      "Epoch 217/499\n",
      "----------\n",
      "train Loss: 0.00237645\n",
      "val Loss: 0.00300368\n",
      "Epoch 218/499\n",
      "----------\n",
      "train Loss: 0.00237371\n",
      "val Loss: 0.00300406\n",
      "Epoch 219/499\n",
      "----------\n",
      "train Loss: 0.00239311\n",
      "val Loss: 0.00300440\n",
      "Epoch 220/499\n",
      "----------\n",
      "train Loss: 0.00237285\n",
      "val Loss: 0.00300429\n",
      "Epoch 221/499\n",
      "----------\n",
      "train Loss: 0.00239985\n",
      "val Loss: 0.00300614\n",
      "Epoch 222/499\n",
      "----------\n",
      "train Loss: 0.00239628\n",
      "val Loss: 0.00300548\n",
      "Epoch 223/499\n",
      "----------\n",
      "train Loss: 0.00239091\n",
      "val Loss: 0.00300608\n",
      "Epoch 224/499\n",
      "----------\n",
      "train Loss: 0.00236013\n",
      "val Loss: 0.00300409\n",
      "Epoch 225/499\n",
      "----------\n",
      "train Loss: 0.00237602\n",
      "val Loss: 0.00300367\n",
      "Epoch 226/499\n",
      "----------\n",
      "train Loss: 0.00237526\n",
      "val Loss: 0.00300289\n",
      "Epoch 227/499\n",
      "----------\n",
      "train Loss: 0.00235490\n",
      "val Loss: 0.00300386\n",
      "Epoch 228/499\n",
      "----------\n",
      "train Loss: 0.00237450\n",
      "val Loss: 0.00300486\n",
      "Epoch 229/499\n",
      "----------\n",
      "train Loss: 0.00237946\n",
      "val Loss: 0.00300799\n",
      "Epoch 230/499\n",
      "----------\n",
      "train Loss: 0.00236817\n",
      "val Loss: 0.00300477\n",
      "Epoch 231/499\n",
      "----------\n",
      "train Loss: 0.00238995\n",
      "val Loss: 0.00300488\n",
      "Epoch 232/499\n",
      "----------\n",
      "train Loss: 0.00234185\n",
      "val Loss: 0.00300351\n",
      "Epoch 233/499\n",
      "----------\n",
      "train Loss: 0.00238545\n",
      "val Loss: 0.00300377\n",
      "Epoch 234/499\n",
      "----------\n",
      "train Loss: 0.00235373\n",
      "val Loss: 0.00300097\n",
      "Epoch 235/499\n",
      "----------\n",
      "train Loss: 0.00238241\n",
      "val Loss: 0.00300116\n",
      "Epoch 236/499\n",
      "----------\n",
      "train Loss: 0.00235327\n",
      "val Loss: 0.00300074\n",
      "Epoch 237/499\n",
      "----------\n",
      "train Loss: 0.00233274\n",
      "val Loss: 0.00300080\n",
      "Epoch 238/499\n",
      "----------\n",
      "train Loss: 0.00236294\n",
      "val Loss: 0.00299971\n",
      "Epoch 239/499\n",
      "----------\n",
      "train Loss: 0.00234170\n",
      "val Loss: 0.00299986\n",
      "Epoch 240/499\n",
      "----------\n",
      "train Loss: 0.00239018\n",
      "val Loss: 0.00300055\n",
      "Epoch 241/499\n",
      "----------\n",
      "train Loss: 0.00236829\n",
      "val Loss: 0.00300266\n",
      "Epoch 242/499\n",
      "----------\n",
      "train Loss: 0.00235949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.00300282\n",
      "Epoch 243/499\n",
      "----------\n",
      "train Loss: 0.00239025\n",
      "val Loss: 0.00300340\n",
      "Epoch 244/499\n",
      "----------\n",
      "train Loss: 0.00236517\n",
      "val Loss: 0.00300150\n",
      "Epoch 245/499\n",
      "----------\n",
      "train Loss: 0.00236713\n",
      "val Loss: 0.00300138\n",
      "Epoch 246/499\n",
      "----------\n",
      "train Loss: 0.00237682\n",
      "val Loss: 0.00300127\n",
      "Epoch 247/499\n",
      "----------\n",
      "train Loss: 0.00236081\n",
      "val Loss: 0.00300219\n",
      "Epoch 248/499\n",
      "----------\n",
      "train Loss: 0.00237723\n",
      "val Loss: 0.00300170\n",
      "Epoch 249/499\n",
      "----------\n",
      "train Loss: 0.00235206\n",
      "val Loss: 0.00300306\n",
      "Epoch 250/499\n",
      "----------\n",
      "train Loss: 0.00237189\n",
      "val Loss: 0.00300287\n",
      "Epoch 251/499\n",
      "----------\n",
      "train Loss: 0.00239061\n",
      "val Loss: 0.00300367\n",
      "Epoch 252/499\n",
      "----------\n",
      "train Loss: 0.00238389\n",
      "val Loss: 0.00300142\n",
      "Epoch 253/499\n",
      "----------\n",
      "train Loss: 0.00236556\n",
      "val Loss: 0.00300279\n",
      "Epoch 254/499\n",
      "----------\n",
      "train Loss: 0.00236854\n",
      "val Loss: 0.00300353\n",
      "Epoch 255/499\n",
      "----------\n",
      "train Loss: 0.00235443\n",
      "val Loss: 0.00300380\n",
      "Epoch 256/499\n",
      "----------\n",
      "train Loss: 0.00233533\n",
      "val Loss: 0.00300247\n",
      "Epoch 257/499\n",
      "----------\n",
      "train Loss: 0.00233308\n",
      "val Loss: 0.00300282\n",
      "Epoch 258/499\n",
      "----------\n",
      "train Loss: 0.00236570\n",
      "val Loss: 0.00300123\n",
      "Epoch 259/499\n",
      "----------\n",
      "train Loss: 0.00236154\n",
      "val Loss: 0.00300125\n",
      "Epoch 260/499\n",
      "----------\n",
      "train Loss: 0.00235111\n",
      "val Loss: 0.00300318\n",
      "Epoch 261/499\n",
      "----------\n",
      "train Loss: 0.00237749\n",
      "val Loss: 0.00300400\n",
      "Epoch 262/499\n",
      "----------\n",
      "train Loss: 0.00235026\n",
      "val Loss: 0.00300331\n",
      "Epoch 263/499\n",
      "----------\n",
      "train Loss: 0.00235558\n",
      "val Loss: 0.00300266\n",
      "Epoch 264/499\n",
      "----------\n",
      "train Loss: 0.00239729\n",
      "val Loss: 0.00300246\n",
      "Epoch 265/499\n",
      "----------\n",
      "train Loss: 0.00239011\n",
      "val Loss: 0.00300247\n",
      "Epoch 266/499\n",
      "----------\n",
      "train Loss: 0.00235814\n",
      "val Loss: 0.00300162\n",
      "Epoch 267/499\n",
      "----------\n",
      "train Loss: 0.00238462\n",
      "val Loss: 0.00300209\n",
      "Epoch 268/499\n",
      "----------\n",
      "train Loss: 0.00237929\n",
      "val Loss: 0.00300157\n",
      "Epoch 269/499\n",
      "----------\n",
      "train Loss: 0.00240258\n",
      "val Loss: 0.00300291\n",
      "Epoch 270/499\n",
      "----------\n",
      "train Loss: 0.00238903\n",
      "val Loss: 0.00300335\n",
      "Epoch 271/499\n",
      "----------\n",
      "train Loss: 0.00239949\n",
      "val Loss: 0.00300499\n",
      "Epoch 272/499\n",
      "----------\n",
      "train Loss: 0.00234522\n",
      "val Loss: 0.00300260\n",
      "Epoch 273/499\n",
      "----------\n",
      "train Loss: 0.00236253\n",
      "val Loss: 0.00300246\n",
      "Epoch 274/499\n",
      "----------\n",
      "train Loss: 0.00237240\n",
      "val Loss: 0.00300293\n",
      "Epoch 275/499\n",
      "----------\n",
      "train Loss: 0.00236283\n",
      "val Loss: 0.00300390\n",
      "Epoch 276/499\n",
      "----------\n",
      "train Loss: 0.00237142\n",
      "val Loss: 0.00300492\n",
      "Epoch 277/499\n",
      "----------\n",
      "train Loss: 0.00235459\n",
      "val Loss: 0.00300290\n",
      "Epoch 278/499\n",
      "----------\n",
      "train Loss: 0.00241634\n",
      "val Loss: 0.00300487\n",
      "Epoch 279/499\n",
      "----------\n",
      "train Loss: 0.00238031\n",
      "val Loss: 0.00300459\n",
      "Epoch 280/499\n",
      "----------\n",
      "train Loss: 0.00237796\n",
      "val Loss: 0.00300518\n",
      "Epoch 281/499\n",
      "----------\n",
      "train Loss: 0.00234157\n",
      "val Loss: 0.00300259\n",
      "Epoch 282/499\n",
      "----------\n",
      "train Loss: 0.00235465\n",
      "val Loss: 0.00300159\n",
      "Epoch 283/499\n",
      "----------\n",
      "train Loss: 0.00237209\n",
      "val Loss: 0.00300171\n",
      "Epoch 284/499\n",
      "----------\n",
      "train Loss: 0.00237079\n",
      "val Loss: 0.00300163\n",
      "Epoch 285/499\n",
      "----------\n",
      "train Loss: 0.00236009\n",
      "val Loss: 0.00300208\n",
      "Epoch 286/499\n",
      "----------\n",
      "train Loss: 0.00233696\n",
      "val Loss: 0.00300068\n",
      "Epoch 287/499\n",
      "----------\n",
      "train Loss: 0.00236552\n",
      "val Loss: 0.00300205\n",
      "Epoch 288/499\n",
      "----------\n",
      "train Loss: 0.00236265\n",
      "val Loss: 0.00300266\n",
      "Epoch 289/499\n",
      "----------\n",
      "train Loss: 0.00238201\n",
      "val Loss: 0.00300409\n",
      "Epoch 290/499\n",
      "----------\n",
      "train Loss: 0.00237097\n",
      "val Loss: 0.00300421\n",
      "Epoch 291/499\n",
      "----------\n",
      "train Loss: 0.00238594\n",
      "val Loss: 0.00300457\n",
      "Epoch 292/499\n",
      "----------\n",
      "train Loss: 0.00239390\n",
      "val Loss: 0.00300513\n",
      "Epoch 293/499\n",
      "----------\n",
      "train Loss: 0.00236325\n",
      "val Loss: 0.00300445\n",
      "Epoch 294/499\n",
      "----------\n",
      "train Loss: 0.00236264\n",
      "val Loss: 0.00300305\n",
      "Epoch 295/499\n",
      "----------\n",
      "train Loss: 0.00240328\n",
      "val Loss: 0.00300322\n",
      "Epoch 296/499\n",
      "----------\n",
      "train Loss: 0.00236612\n",
      "val Loss: 0.00300359\n",
      "Epoch 297/499\n",
      "----------\n",
      "train Loss: 0.00242338\n",
      "val Loss: 0.00300565\n",
      "Epoch 298/499\n",
      "----------\n",
      "train Loss: 0.00237474\n",
      "val Loss: 0.00300476\n",
      "Epoch 299/499\n",
      "----------\n",
      "train Loss: 0.00237691\n",
      "val Loss: 0.00300430\n",
      "Epoch 300/499\n",
      "----------\n",
      "train Loss: 0.00236560\n",
      "val Loss: 0.00300559\n",
      "Epoch 301/499\n",
      "----------\n",
      "train Loss: 0.00239146\n",
      "val Loss: 0.00300513\n",
      "Epoch 302/499\n",
      "----------\n",
      "train Loss: 0.00241433\n",
      "val Loss: 0.00300595\n",
      "Epoch 303/499\n",
      "----------\n",
      "train Loss: 0.00234865\n",
      "val Loss: 0.00300445\n",
      "Epoch 304/499\n",
      "----------\n",
      "train Loss: 0.00238149\n",
      "val Loss: 0.00300512\n",
      "Epoch 305/499\n",
      "----------\n",
      "train Loss: 0.00239576\n",
      "val Loss: 0.00300533\n",
      "Epoch 306/499\n",
      "----------\n",
      "train Loss: 0.00239371\n",
      "val Loss: 0.00300561\n",
      "Epoch 307/499\n",
      "----------\n",
      "train Loss: 0.00237936\n",
      "val Loss: 0.00300434\n",
      "Epoch 308/499\n",
      "----------\n",
      "train Loss: 0.00240827\n",
      "val Loss: 0.00300473\n",
      "Epoch 309/499\n",
      "----------\n",
      "train Loss: 0.00237440\n",
      "val Loss: 0.00300382\n",
      "Epoch 310/499\n",
      "----------\n",
      "train Loss: 0.00238020\n",
      "val Loss: 0.00300209\n",
      "Epoch 311/499\n",
      "----------\n",
      "train Loss: 0.00237194\n",
      "val Loss: 0.00300344\n",
      "Epoch 312/499\n",
      "----------\n",
      "train Loss: 0.00236372\n",
      "val Loss: 0.00300159\n",
      "Epoch 313/499\n",
      "----------\n",
      "train Loss: 0.00238195\n",
      "val Loss: 0.00300248\n",
      "Epoch 314/499\n",
      "----------\n",
      "train Loss: 0.00234232\n",
      "val Loss: 0.00300123\n",
      "Epoch 315/499\n",
      "----------\n",
      "train Loss: 0.00238455\n",
      "val Loss: 0.00300244\n",
      "Epoch 316/499\n",
      "----------\n",
      "train Loss: 0.00236368\n",
      "val Loss: 0.00300178\n",
      "Epoch 317/499\n",
      "----------\n",
      "train Loss: 0.00236964\n",
      "val Loss: 0.00300194\n",
      "Epoch 318/499\n",
      "----------\n",
      "train Loss: 0.00234891\n",
      "val Loss: 0.00300320\n",
      "Epoch 319/499\n",
      "----------\n",
      "train Loss: 0.00239787\n",
      "val Loss: 0.00300363\n",
      "Epoch 320/499\n",
      "----------\n",
      "train Loss: 0.00236983\n",
      "val Loss: 0.00300355\n",
      "Epoch 321/499\n",
      "----------\n",
      "train Loss: 0.00235561\n",
      "val Loss: 0.00300298\n",
      "Epoch 322/499\n",
      "----------\n",
      "train Loss: 0.00238918\n",
      "val Loss: 0.00300492\n",
      "Epoch 323/499\n",
      "----------\n",
      "train Loss: 0.00239496\n",
      "val Loss: 0.00300567\n",
      "Epoch 324/499\n",
      "----------\n",
      "train Loss: 0.00237700\n",
      "val Loss: 0.00300343\n",
      "Epoch 325/499\n",
      "----------\n",
      "train Loss: 0.00238749\n",
      "val Loss: 0.00300294\n",
      "Epoch 326/499\n",
      "----------\n",
      "train Loss: 0.00235524\n",
      "val Loss: 0.00300089\n",
      "Epoch 327/499\n",
      "----------\n",
      "train Loss: 0.00235810\n",
      "val Loss: 0.00300124\n",
      "Epoch 328/499\n",
      "----------\n",
      "train Loss: 0.00236794\n",
      "val Loss: 0.00300118\n",
      "Epoch 329/499\n",
      "----------\n",
      "train Loss: 0.00237347\n",
      "val Loss: 0.00300142\n",
      "Epoch 330/499\n",
      "----------\n",
      "train Loss: 0.00235885\n",
      "val Loss: 0.00300270\n",
      "Epoch 331/499\n",
      "----------\n",
      "train Loss: 0.00235167\n",
      "val Loss: 0.00300117\n",
      "Epoch 332/499\n",
      "----------\n",
      "train Loss: 0.00238091\n",
      "val Loss: 0.00300279\n",
      "Epoch 333/499\n",
      "----------\n",
      "train Loss: 0.00236818\n",
      "val Loss: 0.00300261\n",
      "Epoch 334/499\n",
      "----------\n",
      "train Loss: 0.00242158\n",
      "val Loss: 0.00300430\n",
      "Epoch 335/499\n",
      "----------\n",
      "train Loss: 0.00239192\n",
      "val Loss: 0.00300444\n",
      "Epoch 336/499\n",
      "----------\n",
      "train Loss: 0.00237313\n",
      "val Loss: 0.00300435\n",
      "Epoch 337/499\n",
      "----------\n",
      "train Loss: 0.00235743\n",
      "val Loss: 0.00300408\n",
      "Epoch 338/499\n",
      "----------\n",
      "train Loss: 0.00237938\n",
      "val Loss: 0.00300477\n",
      "Epoch 339/499\n",
      "----------\n",
      "train Loss: 0.00237522\n",
      "val Loss: 0.00300459\n",
      "Epoch 340/499\n",
      "----------\n",
      "train Loss: 0.00234597\n",
      "val Loss: 0.00300555\n",
      "Epoch 341/499\n",
      "----------\n",
      "train Loss: 0.00239027\n",
      "val Loss: 0.00300620\n",
      "Epoch 342/499\n",
      "----------\n",
      "train Loss: 0.00234871\n",
      "val Loss: 0.00300460\n",
      "Epoch 343/499\n",
      "----------\n",
      "train Loss: 0.00234151\n",
      "val Loss: 0.00300286\n",
      "Epoch 344/499\n",
      "----------\n",
      "train Loss: 0.00235615\n",
      "val Loss: 0.00300294\n",
      "Epoch 345/499\n",
      "----------\n",
      "train Loss: 0.00238195\n",
      "val Loss: 0.00300314\n",
      "Epoch 346/499\n",
      "----------\n",
      "train Loss: 0.00239069\n",
      "val Loss: 0.00300372\n",
      "Epoch 347/499\n",
      "----------\n",
      "train Loss: 0.00234528\n",
      "val Loss: 0.00300258\n",
      "Epoch 348/499\n",
      "----------\n",
      "train Loss: 0.00236633\n",
      "val Loss: 0.00300278\n",
      "Epoch 349/499\n",
      "----------\n",
      "train Loss: 0.00237222\n",
      "val Loss: 0.00300337\n",
      "Epoch 350/499\n",
      "----------\n",
      "train Loss: 0.00238044\n",
      "val Loss: 0.00300337\n",
      "Epoch 351/499\n",
      "----------\n",
      "train Loss: 0.00236328\n",
      "val Loss: 0.00300358\n",
      "Epoch 352/499\n",
      "----------\n",
      "train Loss: 0.00240981\n",
      "val Loss: 0.00300389\n",
      "Epoch 353/499\n",
      "----------\n",
      "train Loss: 0.00236444\n",
      "val Loss: 0.00300420\n",
      "Epoch 354/499\n",
      "----------\n",
      "train Loss: 0.00236234\n",
      "val Loss: 0.00300427\n",
      "Epoch 355/499\n",
      "----------\n",
      "train Loss: 0.00235319\n",
      "val Loss: 0.00300267\n",
      "Epoch 356/499\n",
      "----------\n",
      "train Loss: 0.00239341\n",
      "val Loss: 0.00300216\n",
      "Epoch 357/499\n",
      "----------\n",
      "train Loss: 0.00234772\n",
      "val Loss: 0.00300307\n",
      "Epoch 358/499\n",
      "----------\n",
      "train Loss: 0.00237406\n",
      "val Loss: 0.00300409\n",
      "Epoch 359/499\n",
      "----------\n",
      "train Loss: 0.00238247\n",
      "val Loss: 0.00300511\n",
      "Epoch 360/499\n",
      "----------\n",
      "train Loss: 0.00239262\n",
      "val Loss: 0.00300499\n",
      "Epoch 361/499\n",
      "----------\n",
      "train Loss: 0.00235183\n",
      "val Loss: 0.00300548\n",
      "Epoch 362/499\n",
      "----------\n",
      "train Loss: 0.00233332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.00300489\n",
      "Epoch 363/499\n",
      "----------\n",
      "train Loss: 0.00236488\n",
      "val Loss: 0.00300463\n",
      "Epoch 364/499\n",
      "----------\n",
      "train Loss: 0.00238138\n",
      "val Loss: 0.00300554\n",
      "Epoch 365/499\n",
      "----------\n",
      "train Loss: 0.00234251\n",
      "val Loss: 0.00300493\n",
      "Epoch 366/499\n",
      "----------\n",
      "train Loss: 0.00236695\n",
      "val Loss: 0.00300420\n",
      "Epoch 367/499\n",
      "----------\n",
      "train Loss: 0.00238464\n",
      "val Loss: 0.00300515\n",
      "Epoch 368/499\n",
      "----------\n",
      "train Loss: 0.00237343\n",
      "val Loss: 0.00300387\n",
      "Epoch 369/499\n",
      "----------\n",
      "train Loss: 0.00239682\n",
      "val Loss: 0.00300380\n",
      "Epoch 370/499\n",
      "----------\n",
      "train Loss: 0.00236651\n",
      "val Loss: 0.00300459\n",
      "Epoch 371/499\n",
      "----------\n",
      "train Loss: 0.00236379\n",
      "val Loss: 0.00300276\n",
      "Epoch 372/499\n",
      "----------\n",
      "train Loss: 0.00239560\n",
      "val Loss: 0.00300426\n",
      "Epoch 373/499\n",
      "----------\n",
      "train Loss: 0.00237767\n",
      "val Loss: 0.00300471\n",
      "Epoch 374/499\n",
      "----------\n",
      "train Loss: 0.00237079\n",
      "val Loss: 0.00300223\n",
      "Epoch 375/499\n",
      "----------\n",
      "train Loss: 0.00238227\n",
      "val Loss: 0.00300320\n",
      "Epoch 376/499\n",
      "----------\n",
      "train Loss: 0.00236372\n",
      "val Loss: 0.00300131\n",
      "Epoch 377/499\n",
      "----------\n",
      "train Loss: 0.00238532\n",
      "val Loss: 0.00300077\n",
      "Epoch 378/499\n",
      "----------\n",
      "train Loss: 0.00238999\n",
      "val Loss: 0.00300236\n",
      "Epoch 379/499\n",
      "----------\n",
      "train Loss: 0.00239549\n",
      "val Loss: 0.00300281\n",
      "Epoch 380/499\n",
      "----------\n",
      "train Loss: 0.00239743\n",
      "val Loss: 0.00300460\n",
      "Epoch 381/499\n",
      "----------\n",
      "train Loss: 0.00238061\n",
      "val Loss: 0.00300270\n",
      "Epoch 382/499\n",
      "----------\n",
      "train Loss: 0.00238056\n",
      "val Loss: 0.00300219\n",
      "Epoch 383/499\n",
      "----------\n",
      "train Loss: 0.00235913\n",
      "val Loss: 0.00300190\n",
      "Epoch 384/499\n",
      "----------\n",
      "train Loss: 0.00238712\n",
      "val Loss: 0.00300371\n",
      "Epoch 385/499\n",
      "----------\n",
      "train Loss: 0.00235261\n",
      "val Loss: 0.00300380\n",
      "Epoch 386/499\n",
      "----------\n",
      "train Loss: 0.00237384\n",
      "val Loss: 0.00300397\n",
      "Epoch 387/499\n",
      "----------\n",
      "train Loss: 0.00236818\n",
      "val Loss: 0.00300435\n",
      "Epoch 388/499\n",
      "----------\n",
      "train Loss: 0.00236860\n",
      "val Loss: 0.00300350\n",
      "Epoch 389/499\n",
      "----------\n",
      "train Loss: 0.00238119\n",
      "val Loss: 0.00300464\n",
      "Epoch 390/499\n",
      "----------\n",
      "train Loss: 0.00238525\n",
      "val Loss: 0.00300454\n",
      "Epoch 391/499\n",
      "----------\n",
      "train Loss: 0.00239038\n",
      "val Loss: 0.00300425\n",
      "Epoch 392/499\n",
      "----------\n",
      "train Loss: 0.00236460\n",
      "val Loss: 0.00300315\n",
      "Epoch 393/499\n",
      "----------\n",
      "train Loss: 0.00236284\n",
      "val Loss: 0.00300140\n",
      "Epoch 394/499\n",
      "----------\n",
      "train Loss: 0.00239265\n",
      "val Loss: 0.00300123\n",
      "Epoch 395/499\n",
      "----------\n",
      "train Loss: 0.00238234\n",
      "val Loss: 0.00300261\n",
      "Epoch 396/499\n",
      "----------\n",
      "train Loss: 0.00236696\n",
      "val Loss: 0.00300412\n",
      "Epoch 397/499\n",
      "----------\n",
      "train Loss: 0.00233974\n",
      "val Loss: 0.00300155\n",
      "Epoch 398/499\n",
      "----------\n",
      "train Loss: 0.00238971\n",
      "val Loss: 0.00300325\n",
      "Epoch 399/499\n",
      "----------\n",
      "train Loss: 0.00234163\n",
      "val Loss: 0.00300264\n",
      "Epoch 400/499\n",
      "----------\n",
      "train Loss: 0.00239658\n",
      "val Loss: 0.00300328\n",
      "Epoch 401/499\n",
      "----------\n",
      "train Loss: 0.00237301\n",
      "val Loss: 0.00300301\n",
      "Epoch 402/499\n",
      "----------\n",
      "train Loss: 0.00239995\n",
      "val Loss: 0.00300247\n",
      "Epoch 403/499\n",
      "----------\n",
      "train Loss: 0.00236682\n",
      "val Loss: 0.00300248\n",
      "Epoch 404/499\n",
      "----------\n",
      "train Loss: 0.00239953\n",
      "val Loss: 0.00300410\n",
      "Epoch 405/499\n",
      "----------\n",
      "train Loss: 0.00237733\n",
      "val Loss: 0.00300377\n",
      "Epoch 406/499\n",
      "----------\n",
      "train Loss: 0.00233486\n",
      "val Loss: 0.00300216\n",
      "Epoch 407/499\n",
      "----------\n",
      "train Loss: 0.00234886\n",
      "val Loss: 0.00300148\n",
      "Epoch 408/499\n",
      "----------\n",
      "train Loss: 0.00238379\n",
      "val Loss: 0.00300263\n",
      "Epoch 409/499\n",
      "----------\n",
      "train Loss: 0.00238085\n",
      "val Loss: 0.00300209\n",
      "Epoch 410/499\n",
      "----------\n",
      "train Loss: 0.00237199\n",
      "val Loss: 0.00300289\n",
      "Epoch 411/499\n",
      "----------\n",
      "train Loss: 0.00239478\n",
      "val Loss: 0.00300539\n",
      "Epoch 412/499\n",
      "----------\n",
      "train Loss: 0.00235780\n",
      "val Loss: 0.00300359\n",
      "Epoch 413/499\n",
      "----------\n",
      "train Loss: 0.00242444\n",
      "val Loss: 0.00300524\n",
      "Epoch 414/499\n",
      "----------\n",
      "train Loss: 0.00236797\n",
      "val Loss: 0.00300485\n",
      "Epoch 415/499\n",
      "----------\n",
      "train Loss: 0.00237473\n",
      "val Loss: 0.00300453\n",
      "Epoch 416/499\n",
      "----------\n",
      "train Loss: 0.00238351\n",
      "val Loss: 0.00300486\n",
      "Epoch 417/499\n",
      "----------\n",
      "train Loss: 0.00234021\n",
      "val Loss: 0.00300282\n",
      "Epoch 418/499\n",
      "----------\n",
      "train Loss: 0.00237969\n",
      "val Loss: 0.00300399\n",
      "Epoch 419/499\n",
      "----------\n",
      "train Loss: 0.00236195\n",
      "val Loss: 0.00300281\n",
      "Epoch 420/499\n",
      "----------\n",
      "train Loss: 0.00239316\n",
      "val Loss: 0.00300428\n",
      "Epoch 421/499\n",
      "----------\n",
      "train Loss: 0.00239122\n",
      "val Loss: 0.00300567\n",
      "Epoch 422/499\n",
      "----------\n",
      "train Loss: 0.00240535\n",
      "val Loss: 0.00300596\n",
      "Epoch 423/499\n",
      "----------\n",
      "train Loss: 0.00234378\n",
      "val Loss: 0.00300396\n",
      "Epoch 424/499\n",
      "----------\n",
      "train Loss: 0.00238334\n",
      "val Loss: 0.00300406\n",
      "Epoch 425/499\n",
      "----------\n",
      "train Loss: 0.00238676\n",
      "val Loss: 0.00300274\n",
      "Epoch 426/499\n",
      "----------\n",
      "train Loss: 0.00236061\n",
      "val Loss: 0.00300272\n",
      "Epoch 427/499\n",
      "----------\n",
      "train Loss: 0.00237245\n",
      "val Loss: 0.00300418\n",
      "Epoch 428/499\n",
      "----------\n",
      "train Loss: 0.00235082\n",
      "val Loss: 0.00300384\n",
      "Epoch 429/499\n",
      "----------\n",
      "train Loss: 0.00236981\n",
      "val Loss: 0.00300345\n",
      "Epoch 430/499\n",
      "----------\n",
      "train Loss: 0.00237500\n",
      "val Loss: 0.00300222\n",
      "Epoch 431/499\n",
      "----------\n",
      "train Loss: 0.00238462\n",
      "val Loss: 0.00300240\n",
      "Epoch 432/499\n",
      "----------\n",
      "train Loss: 0.00238669\n",
      "val Loss: 0.00300309\n",
      "Epoch 433/499\n",
      "----------\n",
      "train Loss: 0.00236405\n",
      "val Loss: 0.00300275\n",
      "Epoch 434/499\n",
      "----------\n",
      "train Loss: 0.00236414\n",
      "val Loss: 0.00300401\n",
      "Epoch 435/499\n",
      "----------\n",
      "train Loss: 0.00237516\n",
      "val Loss: 0.00300359\n",
      "Epoch 436/499\n",
      "----------\n",
      "train Loss: 0.00236186\n",
      "val Loss: 0.00300279\n",
      "Epoch 437/499\n",
      "----------\n",
      "train Loss: 0.00237684\n",
      "val Loss: 0.00300411\n",
      "Epoch 438/499\n",
      "----------\n",
      "train Loss: 0.00238859\n",
      "val Loss: 0.00300404\n",
      "Epoch 439/499\n",
      "----------\n",
      "train Loss: 0.00237973\n",
      "val Loss: 0.00300434\n",
      "Epoch 440/499\n",
      "----------\n",
      "train Loss: 0.00239649\n",
      "val Loss: 0.00300468\n",
      "Epoch 441/499\n",
      "----------\n",
      "train Loss: 0.00235696\n",
      "val Loss: 0.00300460\n",
      "Epoch 442/499\n",
      "----------\n",
      "train Loss: 0.00237795\n",
      "val Loss: 0.00300475\n",
      "Epoch 443/499\n",
      "----------\n",
      "train Loss: 0.00240848\n",
      "val Loss: 0.00300554\n",
      "Epoch 444/499\n",
      "----------\n",
      "train Loss: 0.00240821\n",
      "val Loss: 0.00300729\n",
      "Epoch 445/499\n",
      "----------\n",
      "train Loss: 0.00236336\n",
      "val Loss: 0.00300664\n",
      "Epoch 446/499\n",
      "----------\n",
      "train Loss: 0.00238971\n",
      "val Loss: 0.00300724\n",
      "Epoch 447/499\n",
      "----------\n",
      "train Loss: 0.00237663\n",
      "val Loss: 0.00300617\n",
      "Epoch 448/499\n",
      "----------\n",
      "train Loss: 0.00237098\n",
      "val Loss: 0.00300543\n",
      "Epoch 449/499\n",
      "----------\n",
      "train Loss: 0.00237748\n",
      "val Loss: 0.00300538\n",
      "Epoch 450/499\n",
      "----------\n",
      "train Loss: 0.00235096\n",
      "val Loss: 0.00300385\n",
      "Epoch 451/499\n",
      "----------\n",
      "train Loss: 0.00237033\n",
      "val Loss: 0.00300381\n",
      "Epoch 452/499\n",
      "----------\n",
      "train Loss: 0.00235294\n",
      "val Loss: 0.00300287\n",
      "Epoch 453/499\n",
      "----------\n",
      "train Loss: 0.00239582\n",
      "val Loss: 0.00300293\n",
      "Epoch 454/499\n",
      "----------\n",
      "train Loss: 0.00235094\n",
      "val Loss: 0.00300101\n",
      "Epoch 455/499\n",
      "----------\n",
      "train Loss: 0.00236215\n",
      "val Loss: 0.00300038\n",
      "Epoch 456/499\n",
      "----------\n",
      "train Loss: 0.00237598\n",
      "val Loss: 0.00300024\n",
      "Epoch 457/499\n",
      "----------\n",
      "train Loss: 0.00238826\n",
      "val Loss: 0.00300185\n",
      "Epoch 458/499\n",
      "----------\n",
      "train Loss: 0.00239961\n",
      "val Loss: 0.00300348\n",
      "Epoch 459/499\n",
      "----------\n",
      "train Loss: 0.00235871\n",
      "val Loss: 0.00300363\n",
      "Epoch 460/499\n",
      "----------\n",
      "train Loss: 0.00239105\n",
      "val Loss: 0.00300506\n",
      "Epoch 461/499\n",
      "----------\n",
      "train Loss: 0.00236007\n",
      "val Loss: 0.00300444\n",
      "Epoch 462/499\n",
      "----------\n",
      "train Loss: 0.00238378\n",
      "val Loss: 0.00300284\n",
      "Epoch 463/499\n",
      "----------\n",
      "train Loss: 0.00237447\n",
      "val Loss: 0.00300315\n",
      "Epoch 464/499\n",
      "----------\n",
      "train Loss: 0.00238884\n",
      "val Loss: 0.00300297\n",
      "Epoch 465/499\n",
      "----------\n",
      "train Loss: 0.00237856\n",
      "val Loss: 0.00300513\n",
      "Epoch 466/499\n",
      "----------\n",
      "train Loss: 0.00233498\n",
      "val Loss: 0.00300314\n",
      "Epoch 467/499\n",
      "----------\n",
      "train Loss: 0.00235899\n",
      "val Loss: 0.00300319\n",
      "Epoch 468/499\n",
      "----------\n",
      "train Loss: 0.00239218\n",
      "val Loss: 0.00300472\n",
      "Epoch 469/499\n",
      "----------\n",
      "train Loss: 0.00239935\n",
      "val Loss: 0.00300698\n",
      "Epoch 470/499\n",
      "----------\n",
      "train Loss: 0.00235124\n",
      "val Loss: 0.00300330\n",
      "Epoch 471/499\n",
      "----------\n",
      "train Loss: 0.00236958\n",
      "val Loss: 0.00300443\n",
      "Epoch 472/499\n",
      "----------\n",
      "train Loss: 0.00239727\n",
      "val Loss: 0.00300574\n",
      "Epoch 473/499\n",
      "----------\n",
      "train Loss: 0.00235501\n",
      "val Loss: 0.00300449\n",
      "Epoch 474/499\n",
      "----------\n",
      "train Loss: 0.00237764\n",
      "val Loss: 0.00300157\n",
      "Epoch 475/499\n",
      "----------\n",
      "train Loss: 0.00239741\n",
      "val Loss: 0.00300306\n",
      "Epoch 476/499\n",
      "----------\n",
      "train Loss: 0.00240538\n",
      "val Loss: 0.00300341\n",
      "Epoch 477/499\n",
      "----------\n",
      "train Loss: 0.00238005\n",
      "val Loss: 0.00300383\n",
      "Epoch 478/499\n",
      "----------\n",
      "train Loss: 0.00239756\n",
      "val Loss: 0.00300308\n",
      "Epoch 479/499\n",
      "----------\n",
      "train Loss: 0.00236933\n",
      "val Loss: 0.00300258\n",
      "Epoch 480/499\n",
      "----------\n",
      "train Loss: 0.00238290\n",
      "val Loss: 0.00300151\n",
      "Epoch 481/499\n",
      "----------\n",
      "train Loss: 0.00237785\n",
      "val Loss: 0.00300120\n",
      "Epoch 482/499\n",
      "----------\n",
      "train Loss: 0.00234937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.00300125\n",
      "Epoch 483/499\n",
      "----------\n",
      "train Loss: 0.00237614\n",
      "val Loss: 0.00300182\n",
      "Epoch 484/499\n",
      "----------\n",
      "train Loss: 0.00238978\n",
      "val Loss: 0.00300207\n",
      "Epoch 485/499\n",
      "----------\n",
      "train Loss: 0.00235152\n",
      "val Loss: 0.00300139\n",
      "Epoch 486/499\n",
      "----------\n",
      "train Loss: 0.00235882\n",
      "val Loss: 0.00300141\n",
      "Epoch 487/499\n",
      "----------\n",
      "train Loss: 0.00234249\n",
      "val Loss: 0.00300275\n",
      "Epoch 488/499\n",
      "----------\n",
      "train Loss: 0.00234595\n",
      "val Loss: 0.00300086\n",
      "Epoch 489/499\n",
      "----------\n",
      "train Loss: 0.00239838\n",
      "val Loss: 0.00300164\n",
      "Epoch 490/499\n",
      "----------\n",
      "train Loss: 0.00238936\n",
      "val Loss: 0.00300218\n",
      "Epoch 491/499\n",
      "----------\n",
      "train Loss: 0.00238117\n",
      "val Loss: 0.00300310\n",
      "Epoch 492/499\n",
      "----------\n",
      "train Loss: 0.00237789\n",
      "val Loss: 0.00300425\n",
      "Epoch 493/499\n",
      "----------\n",
      "train Loss: 0.00236517\n",
      "val Loss: 0.00300451\n",
      "Epoch 494/499\n",
      "----------\n",
      "train Loss: 0.00238074\n",
      "val Loss: 0.00300431\n",
      "Epoch 495/499\n",
      "----------\n",
      "train Loss: 0.00237997\n",
      "val Loss: 0.00300374\n",
      "Epoch 496/499\n",
      "----------\n",
      "train Loss: 0.00239246\n",
      "val Loss: 0.00300293\n",
      "Epoch 497/499\n",
      "----------\n",
      "train Loss: 0.00234506\n",
      "val Loss: 0.00300273\n",
      "Epoch 498/499\n",
      "----------\n",
      "train Loss: 0.00234836\n",
      "val Loss: 0.00300348\n",
      "Epoch 499/499\n",
      "----------\n",
      "train Loss: 0.00238803\n",
      "val Loss: 0.00300409\n"
     ]
    }
   ],
   "source": [
    "model,loss_report = train_ae_model(net=model,data_loaders=dataloaders_train,\n",
    "                             optimizer=optimizer,loss_function=loss_function,\n",
    "                            n_epochs=epochs,scheduler=exp_lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 'train'): 0.002915243928631147,\n",
       " (0, 'val'): 0.003232359610222004,\n",
       " (1, 'train'): 0.002734291332739371,\n",
       " (1, 'val'): 0.003227505694936823,\n",
       " (2, 'train'): 0.0027137183480792576,\n",
       " (2, 'val'): 0.0032197579189583107,\n",
       " (3, 'train'): 0.002724611027925103,\n",
       " (3, 'val'): 0.0032085621798479996,\n",
       " (4, 'train'): 0.002628321310988179,\n",
       " (4, 'val'): 0.0031893167782712866,\n",
       " (5, 'train'): 0.002519234463020607,\n",
       " (5, 'val'): 0.003155301566477175,\n",
       " (6, 'train'): 0.0025244680129819447,\n",
       " (6, 'val'): 0.0031132778083836593,\n",
       " (7, 'train'): 0.002490781730523816,\n",
       " (7, 'val'): 0.0030793682844550523,\n",
       " (8, 'train'): 0.0024758236551726304,\n",
       " (8, 'val'): 0.0030545357752729345,\n",
       " (9, 'train'): 0.0024284741116894614,\n",
       " (9, 'val'): 0.00303453896884565,\n",
       " (10, 'train'): 0.0024239554035442846,\n",
       " (10, 'val'): 0.0030300573066428856,\n",
       " (11, 'train'): 0.00239370404570191,\n",
       " (11, 'val'): 0.003025652633772956,\n",
       " (12, 'train'): 0.0024084121816688115,\n",
       " (12, 'val'): 0.0030221116763574107,\n",
       " (13, 'train'): 0.0024161607854896123,\n",
       " (13, 'val'): 0.0030201676267164724,\n",
       " (14, 'train'): 0.0023827683180570602,\n",
       " (14, 'val'): 0.0030179785357581247,\n",
       " (15, 'train'): 0.0023667908239143865,\n",
       " (15, 'val'): 0.0030151917426674453,\n",
       " (16, 'train'): 0.0023885661115249,\n",
       " (16, 'val'): 0.0030139701233969796,\n",
       " (17, 'train'): 0.0024091054305986122,\n",
       " (17, 'val'): 0.003011380080823545,\n",
       " (18, 'train'): 0.0023918596820698846,\n",
       " (18, 'val'): 0.0030091035145300405,\n",
       " (19, 'train'): 0.0023660816390205313,\n",
       " (19, 'val'): 0.0030047907321541396,\n",
       " (20, 'train'): 0.0023707683991502832,\n",
       " (20, 'val'): 0.003003891695428778,\n",
       " (21, 'train'): 0.0023700533503735505,\n",
       " (21, 'val'): 0.003005241078359109,\n",
       " (22, 'train'): 0.0023651474879847634,\n",
       " (22, 'val'): 0.0030053150322702196,\n",
       " (23, 'train'): 0.0023745156272693916,\n",
       " (23, 'val'): 0.003006024217164075,\n",
       " (24, 'train'): 0.002342324772918666,\n",
       " (24, 'val'): 0.003004042362725293,\n",
       " (25, 'train'): 0.0023883477681212956,\n",
       " (25, 'val'): 0.0030047366464579545,\n",
       " (26, 'train'): 0.002371515871749984,\n",
       " (26, 'val'): 0.0030045583844184875,\n",
       " (27, 'train'): 0.0023806537329046813,\n",
       " (27, 'val'): 0.0030051207652798404,\n",
       " (28, 'train'): 0.0023725897211719442,\n",
       " (28, 'val'): 0.0030048911770184836,\n",
       " (29, 'train'): 0.0023742076009511948,\n",
       " (29, 'val'): 0.003002700706322988,\n",
       " (30, 'train'): 0.00235974643793371,\n",
       " (30, 'val'): 0.0030023386632954635,\n",
       " (31, 'train'): 0.0024060693879922232,\n",
       " (31, 'val'): 0.003003717296653324,\n",
       " (32, 'train'): 0.0023760750061935848,\n",
       " (32, 'val'): 0.003004097276263767,\n",
       " (33, 'train'): 0.002377189419887684,\n",
       " (33, 'val'): 0.0030026068841969527,\n",
       " (34, 'train'): 0.002393312062378283,\n",
       " (34, 'val'): 0.0030035533838801915,\n",
       " (35, 'train'): 0.002350786010976191,\n",
       " (35, 'val'): 0.003003317172880526,\n",
       " (36, 'train'): 0.002360473628397341,\n",
       " (36, 'val'): 0.0030029700310141953,\n",
       " (37, 'train'): 0.002390298854421686,\n",
       " (37, 'val'): 0.0030028734494138647,\n",
       " (38, 'train'): 0.0023566908720466825,\n",
       " (38, 'val'): 0.003003133667839898,\n",
       " (39, 'train'): 0.0023768175807264117,\n",
       " (39, 'val'): 0.0030032948211387352,\n",
       " (40, 'train'): 0.0024052077421435605,\n",
       " (40, 'val'): 0.00300407271694254,\n",
       " (41, 'train'): 0.0023789496195537074,\n",
       " (41, 'val'): 0.0030037426838168393,\n",
       " (42, 'train'): 0.0023577027022838593,\n",
       " (42, 'val'): 0.0030043332113160025,\n",
       " (43, 'train'): 0.002352149329251713,\n",
       " (43, 'val'): 0.003004029945090965,\n",
       " (44, 'train'): 0.0023769687999177862,\n",
       " (44, 'val'): 0.0030046257155912893,\n",
       " (45, 'train'): 0.0023892236942494355,\n",
       " (45, 'val'): 0.0030060871331780044,\n",
       " (46, 'train'): 0.0023772051488911666,\n",
       " (46, 'val'): 0.003005005419254303,\n",
       " (47, 'train'): 0.0023908842079065464,\n",
       " (47, 'val'): 0.0030052308683042174,\n",
       " (48, 'train'): 0.002339618073569404,\n",
       " (48, 'val'): 0.003002731336487664,\n",
       " (49, 'train'): 0.0024061130566729438,\n",
       " (49, 'val'): 0.0030023715010395755,\n",
       " (50, 'train'): 0.002385674872332149,\n",
       " (50, 'val'): 0.0030020894827666104,\n",
       " (51, 'train'): 0.002380147162410948,\n",
       " (51, 'val'): 0.0030027710729175145,\n",
       " (52, 'train'): 0.002340575611149823,\n",
       " (52, 'val'): 0.0030012199724162064,\n",
       " (53, 'train'): 0.00233328846041803,\n",
       " (53, 'val'): 0.0030002754043649744,\n",
       " (54, 'train'): 0.0023565164732712285,\n",
       " (54, 'val'): 0.0030012765416392575,\n",
       " (55, 'train'): 0.002358034942988996,\n",
       " (55, 'val'): 0.0030027771437609635,\n",
       " (56, 'train'): 0.002334483174814118,\n",
       " (56, 'val'): 0.0030015050261108962,\n",
       " (57, 'train'): 0.0024000101343349175,\n",
       " (57, 'val'): 0.0030034449365403918,\n",
       " (58, 'train'): 0.002364526399307781,\n",
       " (58, 'val'): 0.003003624854264436,\n",
       " (59, 'train'): 0.0023731781790653863,\n",
       " (59, 'val'): 0.003003723367496773,\n",
       " (60, 'train'): 0.0023541184211218797,\n",
       " (60, 'val'): 0.0030037424078694095,\n",
       " (61, 'train'): 0.0023928559212772933,\n",
       " (61, 'val'): 0.0030057714493186387,\n",
       " (62, 'train'): 0.002368214988598117,\n",
       " (62, 'val'): 0.0030057093611469974,\n",
       " (63, 'train'): 0.0023316836881416814,\n",
       " (63, 'val'): 0.0030048900732287656,\n",
       " (64, 'train'): 0.0023581746413751884,\n",
       " (64, 'val'): 0.0030039686847616125,\n",
       " (65, 'train'): 0.0023803686102231345,\n",
       " (65, 'val'): 0.0030034380378546536,\n",
       " (66, 'train'): 0.0023623977408364968,\n",
       " (66, 'val'): 0.003001894939828802,\n",
       " (67, 'train'): 0.0024081542398090715,\n",
       " (67, 'val'): 0.003003939986228943,\n",
       " (68, 'train'): 0.002342965660823716,\n",
       " (68, 'val'): 0.00300440302601567,\n",
       " (69, 'train'): 0.0024015003884280167,\n",
       " (69, 'val'): 0.0030038729310035706,\n",
       " (70, 'train'): 0.002386003801668132,\n",
       " (70, 'val'): 0.0030032172799110413,\n",
       " (71, 'train'): 0.0023613869454021806,\n",
       " (71, 'val'): 0.0030026965671115452,\n",
       " (72, 'train'): 0.002395928664891808,\n",
       " (72, 'val'): 0.0030032981325078894,\n",
       " (73, 'train'): 0.0023362009475628534,\n",
       " (73, 'val'): 0.0030005118913120693,\n",
       " (74, 'train'): 0.0023696000377337136,\n",
       " (74, 'val'): 0.0030005888806449044,\n",
       " (75, 'train'): 0.0023691438966327244,\n",
       " (75, 'val'): 0.003000915602401451,\n",
       " (76, 'train'): 0.0023535314809393,\n",
       " (76, 'val'): 0.003000341907695488,\n",
       " (77, 'train'): 0.0023653298202488157,\n",
       " (77, 'val'): 0.0030000965904306482,\n",
       " (78, 'train'): 0.0023647754418629186,\n",
       " (78, 'val'): 0.0030011278059747484,\n",
       " (79, 'train'): 0.002378694161220833,\n",
       " (79, 'val'): 0.0030005886046974746,\n",
       " (80, 'train'): 0.002420686185359955,\n",
       " (80, 'val'): 0.0030017528269026014,\n",
       " (81, 'train'): 0.0023913858803334058,\n",
       " (81, 'val'): 0.0030032051382241427,\n",
       " (82, 'train'): 0.002350408445905756,\n",
       " (82, 'val'): 0.0030033180007228146,\n",
       " (83, 'train'): 0.0023946580649526032,\n",
       " (83, 'val'): 0.003004333763210862,\n",
       " (84, 'train'): 0.002351023739686719,\n",
       " (84, 'val'): 0.003002674491317184,\n",
       " (85, 'train'): 0.0023804891992498327,\n",
       " (85, 'val'): 0.003004352527636069,\n",
       " (86, 'train'): 0.002384091141047301,\n",
       " (86, 'val'): 0.003003859685526954,\n",
       " (87, 'train'): 0.0023511029365989897,\n",
       " (87, 'val'): 0.003002611299355825,\n",
       " (88, 'train'): 0.002373436672819985,\n",
       " (88, 'val'): 0.0030033842281058983,\n",
       " (89, 'train'): 0.002360998342434565,\n",
       " (89, 'val'): 0.0030034689439667594,\n",
       " (90, 'train'): 0.0023903193435183276,\n",
       " (90, 'val'): 0.0030050810288499903,\n",
       " (91, 'train'): 0.0024039775685027794,\n",
       " (91, 'val'): 0.003005906111664242,\n",
       " (92, 'train'): 0.0023828282676361225,\n",
       " (92, 'val'): 0.003006276433114652,\n",
       " (93, 'train'): 0.0023970207958309737,\n",
       " (93, 'val'): 0.003007559036767041,\n",
       " (94, 'train'): 0.0023855153057310316,\n",
       " (94, 'val'): 0.0030059165976665638,\n",
       " (95, 'train'): 0.0023755176613728204,\n",
       " (95, 'val'): 0.0030044005424888047,\n",
       " (96, 'train'): 0.002351237185023449,\n",
       " (96, 'val'): 0.003004489673508538,\n",
       " (97, 'train'): 0.0023697511189513737,\n",
       " (97, 'val'): 0.0030022586385409036,\n",
       " (98, 'train'): 0.002381975245144632,\n",
       " (98, 'val'): 0.003002898284682521,\n",
       " (99, 'train'): 0.002356944191786978,\n",
       " (99, 'val'): 0.003002036224912714,\n",
       " (100, 'train'): 0.0023733716872003344,\n",
       " (100, 'val'): 0.003001121735131299,\n",
       " (101, 'train'): 0.0024077165871858597,\n",
       " (101, 'val'): 0.0030019493014724168,\n",
       " (102, 'train'): 0.0023616279854818626,\n",
       " (102, 'val'): 0.003002866274780697,\n",
       " (103, 'train'): 0.002359628815341879,\n",
       " (103, 'val'): 0.0030016780451492028,\n",
       " (104, 'train'): 0.0023881325291262735,\n",
       " (104, 'val'): 0.003002874829151012,\n",
       " (105, 'train'): 0.0023823450836870405,\n",
       " (105, 'val'): 0.003003440797328949,\n",
       " (106, 'train'): 0.002349811295668284,\n",
       " (106, 'val'): 0.003000664214293162,\n",
       " (107, 'train'): 0.002385180512512172,\n",
       " (107, 'val'): 0.0030018830740893327,\n",
       " (108, 'train'): 0.0023706275279875154,\n",
       " (108, 'val'): 0.003001555524490498,\n",
       " (109, 'train'): 0.0023764220790730584,\n",
       " (109, 'val'): 0.0030017898038581566,\n",
       " (110, 'train'): 0.0023670984363114394,\n",
       " (110, 'val'): 0.003001700120943564,\n",
       " (111, 'train'): 0.002363986991069935,\n",
       " (111, 'val'): 0.003001185478987517,\n",
       " (112, 'train'): 0.0023621561488619556,\n",
       " (112, 'val'): 0.0030019424027866786,\n",
       " (113, 'train'): 0.0023790179855293697,\n",
       " (113, 'val'): 0.0030018138112845242,\n",
       " (114, 'train'): 0.002382278718330242,\n",
       " (114, 'val'): 0.0030021932390001086,\n",
       " (115, 'train'): 0.0023549188756280476,\n",
       " (115, 'val'): 0.00300252299617838,\n",
       " (116, 'train'): 0.0023610295934809577,\n",
       " (116, 'val'): 0.003001207278834449,\n",
       " (117, 'train'): 0.002344927164139571,\n",
       " (117, 'val'): 0.0030006145437558493,\n",
       " (118, 'train'): 0.00237710608376397,\n",
       " (118, 'val'): 0.0029999933860920093,\n",
       " (119, 'train'): 0.0023410519654000245,\n",
       " (119, 'val'): 0.0030005419695818864,\n",
       " (120, 'train'): 0.0023818435492338956,\n",
       " (120, 'val'): 0.0030019702734770596,\n",
       " (121, 'train'): 0.002334676889909638,\n",
       " (121, 'val'): 0.0030009296757203562,\n",
       " (122, 'train'): 0.002350807327915121,\n",
       " (122, 'val'): 0.003001205623149872,\n",
       " (123, 'train'): 0.0024016723036766052,\n",
       " (123, 'val'): 0.0030035417940881518,\n",
       " (124, 'train'): 0.0023711871493745733,\n",
       " (124, 'val'): 0.0030031322881027504,\n",
       " (125, 'train'): 0.0023664981816653853,\n",
       " (125, 'val'): 0.0030026791824234855,\n",
       " (126, 'train'): 0.0023920671945368804,\n",
       " (126, 'val'): 0.003004639512962765,\n",
       " (127, 'train'): 0.0023524718428099595,\n",
       " (127, 'val'): 0.0030034198253243057,\n",
       " (128, 'train'): 0.002366783787254934,\n",
       " (128, 'val'): 0.003003187753536083,\n",
       " (129, 'train'): 0.002369974429408709,\n",
       " (129, 'val'): 0.003003689701910372,\n",
       " (130, 'train'): 0.002363774166614921,\n",
       " (130, 'val'): 0.003003330970252002,\n",
       " (131, 'train'): 0.0023546106423492784,\n",
       " (131, 'val'): 0.003002387230043058,\n",
       " (132, 'train'): 0.002394933322513545,\n",
       " (132, 'val'): 0.0030021824770503575,\n",
       " (133, 'train'): 0.0024013240580205565,\n",
       " (133, 'val'): 0.003003032947028125,\n",
       " (134, 'train'): 0.0023604732834630543,\n",
       " (134, 'val'): 0.003001692118468108,\n",
       " (135, 'train'): 0.0023758832227300714,\n",
       " (135, 'val'): 0.003000910911295149,\n",
       " (136, 'train'): 0.0023553877103107946,\n",
       " (136, 'val'): 0.0030009465085135568,\n",
       " (137, 'train'): 0.0023957128050150693,\n",
       " (137, 'val'): 0.0030025654920825253,\n",
       " (138, 'train'): 0.002368114819681203,\n",
       " (138, 'val'): 0.0030029305705317746,\n",
       " (139, 'train'): 0.002357548792605047,\n",
       " (139, 'val'): 0.0030014928844239977,\n",
       " (140, 'train'): 0.002385595606433021,\n",
       " (140, 'val'): 0.003003358013100094,\n",
       " (141, 'train'): 0.002367483313988756,\n",
       " (141, 'val'): 0.003004279953462106,\n",
       " (142, 'train'): 0.002388239941663212,\n",
       " (142, 'val'): 0.00300431196336393,\n",
       " (143, 'train'): 0.0024252881606419883,\n",
       " (143, 'val'): 0.0030063178252290796,\n",
       " (144, 'train'): 0.002341966110247153,\n",
       " (144, 'val'): 0.0030054345175072,\n",
       " (145, 'train'): 0.002394532439885316,\n",
       " (145, 'val'): 0.003005646445133068,\n",
       " (146, 'train'): 0.0023766689830356175,\n",
       " (146, 'val'): 0.003004047605726454,\n",
       " (147, 'train'): 0.002367989263600773,\n",
       " (147, 'val'): 0.003004989414303391,\n",
       " (148, 'train'): 0.0023904412432953163,\n",
       " (148, 'val'): 0.0030042192450276126,\n",
       " (149, 'train'): 0.002343127986899129,\n",
       " (149, 'val'): 0.003003966753129606,\n",
       " (150, 'train'): 0.002367141898031588,\n",
       " (150, 'val'): 0.0030025196848092258,\n",
       " (151, 'train'): 0.0023675938309342774,\n",
       " (151, 'val'): 0.0030011689221417464,\n",
       " (152, 'train'): 0.0023600325264312603,\n",
       " (152, 'val'): 0.0030020566450224984,\n",
       " (153, 'train'): 0.002389774347345034,\n",
       " (153, 'val'): 0.0030023163115536727,\n",
       " (154, 'train'): 0.0024014025650642536,\n",
       " (154, 'val'): 0.0030026463446793735,\n",
       " (155, 'train'): 0.00237032419277562,\n",
       " (155, 'val'): 0.0030015596637019406,\n",
       " (156, 'train'): 0.002369101607689151,\n",
       " (156, 'val'): 0.003002149087411386,\n",
       " (157, 'train'): 0.0023932399711123218,\n",
       " (157, 'val'): 0.0030034896400239733,\n",
       " (158, 'train'): 0.0023431655847364003,\n",
       " (158, 'val'): 0.003001526550010399,\n",
       " (159, 'train'): 0.0023666970707752087,\n",
       " (159, 'val'): 0.0030017859405941432,\n",
       " (160, 'train'): 0.0023885819095152395,\n",
       " (160, 'val'): 0.0030006920849835433,\n",
       " (161, 'train'): 0.0023596824181300624,\n",
       " (161, 'val'): 0.0030007784565289817,\n",
       " (162, 'train'): 0.0023657051087529572,\n",
       " (162, 'val'): 0.0030016689388840286,\n",
       " (163, 'train'): 0.0023690678041290354,\n",
       " (163, 'val'): 0.0030020422957561634,\n",
       " (164, 'train'): 0.002368924104505115,\n",
       " (164, 'val'): 0.0030016634199354383,\n",
       " (165, 'train'): 0.002370063974349587,\n",
       " (165, 'val'): 0.003002374260513871,\n",
       " (166, 'train'): 0.002394955329321049,\n",
       " (166, 'val'): 0.003002760862862622,\n",
       " (167, 'train'): 0.002354886727752509,\n",
       " (167, 'val'): 0.0030022934079170227,\n",
       " (168, 'train'): 0.002360299712529889,\n",
       " (168, 'val'): 0.0030031938243795324,\n",
       " (169, 'train'): 0.0023628956189862002,\n",
       " (169, 'val'): 0.0030040426386727225,\n",
       " (170, 'train'): 0.0024313287188609443,\n",
       " (170, 'val'): 0.003004537688361274,\n",
       " (171, 'train'): 0.00236086161048324,\n",
       " (171, 'val'): 0.0030045004354582894,\n",
       " (172, 'train'): 0.0023889305690924325,\n",
       " (172, 'val'): 0.0030063846045070225,\n",
       " (173, 'train'): 0.002354233836134275,\n",
       " (173, 'val'): 0.003004114385004397,\n",
       " (174, 'train'): 0.002382063755282649,\n",
       " (174, 'val'): 0.0030042393891899673,\n",
       " (175, 'train'): 0.0023398466960147576,\n",
       " (175, 'val'): 0.0030020370527550026,\n",
       " (176, 'train'): 0.0023754737857315275,\n",
       " (176, 'val'): 0.0030023933008865075,\n",
       " (177, 'train'): 0.0023544661148830696,\n",
       " (177, 'val'): 0.0030020825840808727,\n",
       " (178, 'train'): 0.002374205324384901,\n",
       " (178, 'val'): 0.0030031237337324354,\n",
       " (179, 'train'): 0.002355142117098526,\n",
       " (179, 'val'): 0.0030026579344714127,\n",
       " (180, 'train'): 0.0023417501123966992,\n",
       " (180, 'val'): 0.0030019749645833617,\n",
       " (181, 'train'): 0.002377738141351276,\n",
       " (181, 'val'): 0.003002503679858314,\n",
       " (182, 'train'): 0.0023761674485824726,\n",
       " (182, 'val'): 0.0030021397051987826,\n",
       " (183, 'train'): 0.002360999722171713,\n",
       " (183, 'val'): 0.003001644931457661,\n",
       " (184, 'train'): 0.002352757586373223,\n",
       " (184, 'val'): 0.003001495092003434,\n",
       " (185, 'train'): 0.0023837564858021558,\n",
       " (185, 'val'): 0.0030001346711759215,\n",
       " (186, 'train'): 0.0024061641759342616,\n",
       " (186, 'val'): 0.003002090310608899,\n",
       " (187, 'train'): 0.002338162726826138,\n",
       " (187, 'val'): 0.0030010571634327926,\n",
       " (188, 'train'): 0.0023572000640409963,\n",
       " (188, 'val'): 0.003002189375736095,\n",
       " (189, 'train'): 0.002403841388446313,\n",
       " (189, 'val'): 0.003003317724775385,\n",
       " (190, 'train'): 0.0023612401413696782,\n",
       " (190, 'val'): 0.003003654932534253,\n",
       " (191, 'train'): 0.0024148756293235004,\n",
       " (191, 'val'): 0.0030058473348617554,\n",
       " (192, 'train'): 0.0023716068654148666,\n",
       " (192, 'val'): 0.0030036461022165087,\n",
       " (193, 'train'): 0.0023528588590798556,\n",
       " (193, 'val'): 0.003004386469169899,\n",
       " (194, 'train'): 0.0023859105314369554,\n",
       " (194, 'val'): 0.003003861341211531,\n",
       " (195, 'train'): 0.0023578099768470835,\n",
       " (195, 'val'): 0.003002668144526305,\n",
       " (196, 'train'): 0.0023985603755270995,\n",
       " (196, 'val'): 0.0030029195326345937,\n",
       " (197, 'train'): 0.002382793567246861,\n",
       " (197, 'val'): 0.003003013354760629,\n",
       " (198, 'train'): 0.0023821655798841406,\n",
       " (198, 'val'): 0.0030037170207058944,\n",
       " (199, 'train'): 0.00234594051208761,\n",
       " (199, 'val'): 0.0030041886148629367,\n",
       " (200, 'train'): 0.0023855295170236517,\n",
       " (200, 'val'): 0.0030037926303015817,\n",
       " (201, 'train'): 0.0023775554641529365,\n",
       " (201, 'val'): 0.0030036759045388964,\n",
       " (202, 'train'): 0.002368346477548281,\n",
       " (202, 'val'): 0.0030041008635803505,\n",
       " (203, 'train'): 0.0023753674769843064,\n",
       " (203, 'val'): 0.003004489949455968,\n",
       " (204, 'train'): 0.002387150294250912,\n",
       " (204, 'val'): 0.00300389611058765,\n",
       " (205, 'train'): 0.0023761486841572654,\n",
       " (205, 'val'): 0.003002663729367433,\n",
       " (206, 'train'): 0.0023587625473737717,\n",
       " (206, 'val'): 0.003002197102264122,\n",
       " (207, 'train'): 0.002373571197191874,\n",
       " (207, 'val'): 0.003003165677741722,\n",
       " (208, 'train'): 0.002360755577683449,\n",
       " (208, 'val'): 0.0030042192450276126,\n",
       " (209, 'train'): 0.00236860524725031,\n",
       " (209, 'val'): 0.003004461250923298,\n",
       " (210, 'train'): 0.002362157459612246,\n",
       " (210, 'val'): 0.0030042645004060534,\n",
       " (211, 'train'): 0.0023713710683363454,\n",
       " (211, 'val'): 0.0030041066584763705,\n",
       " (212, 'train'): 0.0023869621670908397,\n",
       " (212, 'val'): 0.0030048274331622655,\n",
       " (213, 'train'): 0.002355931740668085,\n",
       " (213, 'val'): 0.0030030439849253053,\n",
       " (214, 'train'): 0.0023742031857923226,\n",
       " (214, 'val'): 0.003003494883025134,\n",
       " (215, 'train'): 0.002403484519433092,\n",
       " (215, 'val'): 0.003003860237421813,\n",
       " (216, 'train'): 0.0023661969850460687,\n",
       " (216, 'val'): 0.0030037608963471873,\n",
       " (217, 'train'): 0.0023736191430577527,\n",
       " (217, 'val'): 0.003003483017285665,\n",
       " (218, 'train'): 0.00237727006552396,\n",
       " (218, 'val'): 0.003004024150194945,\n",
       " (219, 'train'): 0.0024147878090540567,\n",
       " (219, 'val'): 0.0030057043940932664,\n",
       " (220, 'train'): 0.0023639304908337415,\n",
       " (220, 'val'): 0.0030036273377913015,\n",
       " (221, 'train'): 0.0023612762214960874,\n",
       " (221, 'val'): 0.0030023841946213332,\n",
       " (222, 'train'): 0.002355709051092466,\n",
       " (222, 'val'): 0.003000327006534294,\n",
       " (223, 'train'): 0.002360427200242325,\n",
       " (223, 'val'): 0.003000366742964144,\n",
       " (224, 'train'): 0.002354043432407909,\n",
       " (224, 'val'): 0.0030012409444208498,\n",
       " (225, 'train'): 0.0024161977624451675,\n",
       " (225, 'val'): 0.003003503437395449,\n",
       " (226, 'train'): 0.0023806015098536454,\n",
       " (226, 'val'): 0.0030031322881027504,\n",
       " (227, 'train'): 0.0023626545099196612,\n",
       " (227, 'val'): 0.003003031843238407,\n",
       " (228, 'train'): 0.002391118487274205,\n",
       " (228, 'val'): 0.003003378157262449,\n",
       " (229, 'train'): 0.0023570863737000358,\n",
       " (229, 'val'): 0.003001963374791322,\n",
       " (230, 'train'): 0.0023726344936423832,\n",
       " (230, 'val'): 0.003003284887031273,\n",
       " (231, 'train'): 0.0023654059127525048,\n",
       " (231, 'val'): 0.003003592292467753,\n",
       " (232, 'train'): 0.0023575731449657017,\n",
       " (232, 'val'): 0.003003957646864432,\n",
       " (233, 'train'): 0.0023623012282230236,\n",
       " (233, 'val'): 0.003003326831040559,\n",
       " (234, 'train'): 0.0023402461989058387,\n",
       " (234, 'val'): 0.00300227215996495,\n",
       " (235, 'train'): 0.002383649625160076,\n",
       " (235, 'val'): 0.00300244876631984,\n",
       " (236, 'train'): 0.002349322661757469,\n",
       " (236, 'val'): 0.003001373675134447,\n",
       " (237, 'train'): 0.0023747301074089826,\n",
       " (237, 'val'): 0.003001296133906753,\n",
       " (238, 'train'): 0.002367457236956667,\n",
       " (238, 'val'): 0.003000452286667294,\n",
       " (239, 'train'): 0.0023997537791728973,\n",
       " (239, 'val'): 0.003002148535516527,\n",
       " (240, 'train'): 0.0023831009726833414,\n",
       " (240, 'val'): 0.0030043977830145094,\n",
       " (241, 'train'): 0.0023835609080614865,\n",
       " (241, 'val'): 0.003004470357188472,\n",
       " (242, 'train'): 0.002344078211872666,\n",
       " (242, 'val'): 0.0030015883622346103,\n",
       " (243, 'train'): 0.0024120862147322406,\n",
       " (243, 'val'): 0.0030034432808558145,\n",
       " (244, 'train'): 0.002325011969164566,\n",
       " (244, 'val'): 0.0030008667597064267,\n",
       " (245, 'train'): 0.0023487993274573927,\n",
       " (245, 'val'): 0.0029998098810513816,\n",
       " (246, 'train'): 0.0023616417828533384,\n",
       " (246, 'val'): 0.0030013504955503675,\n",
       " (247, 'train'): 0.002375545256115772,\n",
       " (247, 'val'): 0.003002424758893472,\n",
       " (248, 'train'): 0.0023732118446517874,\n",
       " (248, 'val'): 0.003004102795212357,\n",
       " (249, 'train'): 0.0023868561342910485,\n",
       " (249, 'val'): 0.0030031466373690854,\n",
       " (250, 'train'): 0.0023690380018066477,\n",
       " (250, 'val'): 0.0030025246518629567,\n",
       " (251, 'train'): 0.0023901536370869035,\n",
       " (251, 'val'): 0.003002046986862465,\n",
       " (252, 'train'): 0.0023999551518095862,\n",
       " (252, 'val'): 0.003003564421777372,\n",
       " (253, 'train'): 0.0023748182036258557,\n",
       " (253, 'val'): 0.0030033886432647705,\n",
       " (254, 'train'): 0.0023880748561135043,\n",
       " (254, 'val'): 0.0030039041130631057,\n",
       " (255, 'train'): 0.002369193843117467,\n",
       " (255, 'val'): 0.0030026416535730714,\n",
       " (256, 'train'): 0.0023409406895990724,\n",
       " (256, 'val'): 0.0030034421770660964,\n",
       " (257, 'train'): 0.0024049231713568725,\n",
       " (257, 'val'): 0.0030037048790189954,\n",
       " (258, 'train'): 0.002393381118222519,\n",
       " (258, 'val'): 0.003004806461157622,\n",
       " (259, 'train'): 0.0023756752963419313,\n",
       " (259, 'val'): 0.0030042774699352405,\n",
       " (260, 'train'): 0.0023985058069229126,\n",
       " (260, 'val'): 0.003006529200960089,\n",
       " (261, 'train'): 0.0023682672806360104,\n",
       " (261, 'val'): 0.003005534686424114,\n",
       " (262, 'train'): 0.0023709447985446014,\n",
       " (262, 'val'): 0.0030060438094315707,\n",
       " (263, 'train'): 0.0023699759471195714,\n",
       " (263, 'val'): 0.0030055087473657397,\n",
       " (264, 'train'): 0.002383188103084211,\n",
       " (264, 'val'): 0.003006143702401055,\n",
       " (265, 'train'): 0.002395872164655615,\n",
       " (265, 'val'): 0.0030056610703468323,\n",
       " (266, 'train'): 0.0024098747030452446,\n",
       " (266, 'val'): 0.00300586196007552,\n",
       " (267, 'train'): 0.002383780493228524,\n",
       " (267, 'val'): 0.003006694217522939,\n",
       " (268, 'train'): 0.002369196671578619,\n",
       " (268, 'val'): 0.0030060647814362136,\n",
       " (269, 'train'): 0.0023513593607478673,\n",
       " (269, 'val'): 0.0030041966173383924,\n",
       " (270, 'train'): 0.002373725382818116,\n",
       " (270, 'val'): 0.003003919566119159,\n",
       " (271, 'train'): 0.0023950533596453845,\n",
       " (271, 'val'): 0.0030030141826029176,\n",
       " (272, 'train'): 0.002377648596410398,\n",
       " (272, 'val'): 0.0030033395246223168,\n",
       " (273, 'train'): 0.0023941674994097818,\n",
       " (273, 'val'): 0.0030024104096271374,\n",
       " (274, 'train'): 0.0023834864022555173,\n",
       " (274, 'val'): 0.003002325141871417,\n",
       " (275, 'train'): 0.002363508015319153,\n",
       " (275, 'val'): 0.0029994999920880355,\n",
       " (276, 'train'): 0.002391610846475319,\n",
       " (276, 'val'): 0.0030018637577692666,\n",
       " (277, 'train'): 0.002392694561017884,\n",
       " (277, 'val'): 0.003002049470389331,\n",
       " (278, 'train'): 0.0023750023295482,\n",
       " (278, 'val'): 0.003002691600057814,\n",
       " (279, 'train'): 0.0023986403312948015,\n",
       " (279, 'val'): 0.0030042934748861524,\n",
       " (280, 'train'): 0.0023572638078972145,\n",
       " (280, 'val'): 0.003002969479119336,\n",
       " (281, 'train'): 0.0023659714670092973,\n",
       " (281, 'val'): 0.0030030332229755543,\n",
       " (282, 'train'): 0.0023868301262458167,\n",
       " (282, 'val'): 0.003005085168061433,\n",
       " (283, 'train'): 0.0023485422134399414,\n",
       " (283, 'val'): 0.003003779660772394,\n",
       " (284, 'train'): 0.002351882350113657,\n",
       " (284, 'val'): 0.0030029474033249747,\n",
       " (285, 'train'): 0.002347337288988961,\n",
       " (285, 'val'): 0.003002440763844384,\n",
       " (286, 'train'): 0.002353097691580101,\n",
       " (286, 'val'): 0.003003619059368416,\n",
       " (287, 'train'): 0.0023676334293904126,\n",
       " (287, 'val'): 0.003003641963005066,\n",
       " (288, 'train'): 0.002370889678045555,\n",
       " (288, 'val'): 0.00300500128004286,\n",
       " (289, 'train'): 0.0023494272458332555,\n",
       " (289, 'val'): 0.0030033323499891493,\n",
       " (290, 'train'): 0.0023600501870667495,\n",
       " (290, 'val'): 0.00300329758061303,\n",
       " (291, 'train'): 0.002351448698728173,\n",
       " (291, 'val'): 0.003003215900173894,\n",
       " (292, 'train'): 0.0023587545448983155,\n",
       " (292, 'val'): 0.0030021466038845203,\n",
       " (293, 'train'): 0.002405655121913663,\n",
       " (293, 'val'): 0.0030038182934125266,\n",
       " (294, 'train'): 0.0024011520047982535,\n",
       " (294, 'val'): 0.0030048255015302588,\n",
       " (295, 'train'): 0.0023691508643053196,\n",
       " (295, 'val'): 0.0030041394962204825,\n",
       " (296, 'train'): 0.0024080367551909555,\n",
       " (296, 'val'): 0.0030057800036889537,\n",
       " (297, 'train'): 0.0023742356096152907,\n",
       " (297, 'val'): 0.0030063305188108374,\n",
       " (298, 'train'): 0.002373654602302445,\n",
       " (298, 'val'): 0.003005340419433735,\n",
       " (299, 'train'): 0.0023460805554080893,\n",
       " (299, 'val'): 0.0030041414278524892,\n",
       " (300, 'train'): 0.002406256549336292,\n",
       " (300, 'val'): 0.003004092585157465,\n",
       " (301, 'train'): 0.0023977368104237096,\n",
       " (301, 'val'): 0.003005127939913008,\n",
       " (302, 'train'): 0.002381959033233148,\n",
       " (302, 'val'): 0.0030051042084340697,\n",
       " (303, 'train'): 0.0023547697950292517,\n",
       " (303, 'val'): 0.0030037642077163414,\n",
       " (304, 'train'): 0.00238673264781634,\n",
       " (304, 'val'): 0.0030048489570617676,\n",
       " (305, 'train'): 0.002365101404764034,\n",
       " (305, 'val'): 0.0030035658015145194,\n",
       " (306, 'train'): 0.0023659776068396038,\n",
       " (306, 'val'): 0.0030034843970228125,\n",
       " (307, 'train'): 0.0023510524382193885,\n",
       " (307, 'val'): 0.00300251140638634,\n",
       " (308, 'train'): 0.0023763765477471882,\n",
       " (308, 'val'): 0.0030042551181934498,\n",
       " (309, 'train'): 0.002366029484956353,\n",
       " (309, 'val'): 0.0030037446154488456,\n",
       " (310, 'train'): 0.0023648269060585233,\n",
       " (310, 'val'): 0.0030041944097589563,\n",
       " (311, 'train'): 0.0023642832896223773,\n",
       " (311, 'val'): 0.0030035989152060617,\n",
       " (312, 'train'): 0.00235537887999305,\n",
       " (312, 'val'): 0.0030025572136596398,\n",
       " (313, 'train'): 0.002380714648299747,\n",
       " (313, 'val'): 0.0030020908625037583,\n",
       " (314, 'train'): 0.002359501465603157,\n",
       " (314, 'val'): 0.0030032895781375744,\n",
       " (315, 'train'): 0.002408498208279963,\n",
       " (315, 'val'): 0.0030029220161614596,\n",
       " (316, 'train'): 0.002364503495671131,\n",
       " (316, 'val'): 0.0030039319837534867,\n",
       " (317, 'train'): 0.0023771672751064653,\n",
       " (317, 'val'): 0.0030013160021216782,\n",
       " (318, 'train'): 0.0023950288003241576,\n",
       " (318, 'val'): 0.0030030969668317724,\n",
       " (319, 'train'): 0.0023692521370119518,\n",
       " (319, 'val'): 0.003002293683864452,\n",
       " (320, 'train'): 0.00237417745369452,\n",
       " (320, 'val'): 0.0030018844538264805,\n",
       " (321, 'train'): 0.0023669514253183647,\n",
       " (321, 'val'): 0.0030024440752135385,\n",
       " (322, 'train'): 0.002365441992878914,\n",
       " (322, 'val'): 0.0030019744126885024,\n",
       " (323, 'train'): 0.0024073833116778623,\n",
       " (323, 'val'): 0.0030031720245326005,\n",
       " (324, 'train'): 0.002386089897266141,\n",
       " (324, 'val'): 0.0030050824085871377,\n",
       " (325, 'train'): 0.00236376340466517,\n",
       " (325, 'val'): 0.0030046519305970934,\n",
       " (326, 'train'): 0.002397807659926238,\n",
       " (326, 'val'): 0.0030049987965159947,\n",
       " (327, 'train'): 0.0023769530709143037,\n",
       " (327, 'val'): 0.0030055959467534667,\n",
       " (328, 'train'): 0.0023472679571972955,\n",
       " (328, 'val'): 0.0030044551800798487,\n",
       " (329, 'train'): 0.002379647973510954,\n",
       " (329, 'val'): 0.0030050374291561268,\n",
       " (330, 'train'): 0.0023808572441339493,\n",
       " (330, 'val'): 0.003005627404760431,\n",
       " (331, 'train'): 0.0023545419314393292,\n",
       " (331, 'val'): 0.0030044295169689038,\n",
       " (332, 'train'): 0.0023578675808729947,\n",
       " (332, 'val'): 0.0030036621071674206,\n",
       " (333, 'train'): 0.0023783434320379187,\n",
       " (333, 'val'): 0.003003279368082682,\n",
       " (334, 'train'): 0.0023978625734647117,\n",
       " (334, 'val'): 0.0030034424530135263,\n",
       " (335, 'train'): 0.0023576328185973346,\n",
       " (335, 'val'): 0.003002670352105741,\n",
       " (336, 'train'): 0.0023668907858707287,\n",
       " (336, 'val'): 0.0030033577371526648,\n",
       " (337, 'train'): 0.0023784311143336474,\n",
       " (337, 'val'): 0.0030048853821224636,\n",
       " (338, 'train'): 0.0023937358486431615,\n",
       " (338, 'val'): 0.0030050250115217984,\n",
       " (339, 'train'): 0.002361455104417271,\n",
       " (339, 'val'): 0.003005664657663416,\n",
       " (340, 'train'): 0.0023470163621284344,\n",
       " (340, 'val'): 0.0030036477579010856,\n",
       " (341, 'train'): 0.002405203464958403,\n",
       " (341, 'val'): 0.0030031413943679246,\n",
       " (342, 'train'): 0.0023878848663082828,\n",
       " (342, 'val'): 0.003005654447608524,\n",
       " (343, 'train'): 0.0023814339052747797,\n",
       " (343, 'val'): 0.0030066130889786612,\n",
       " (344, 'train'): 0.002347506375776397,\n",
       " (344, 'val'): 0.0030062193119967423,\n",
       " (345, 'train'): 0.002341858697710214,\n",
       " (345, 'val'): 0.0030043094798370643,\n",
       " (346, 'train'): 0.0023595445823890193,\n",
       " (346, 'val'): 0.003005060608740206,\n",
       " (347, 'train'): 0.0023866221998576766,\n",
       " (347, 'val'): 0.0030036209910004226,\n",
       " (348, 'train'): 0.00234473137943833,\n",
       " (348, 'val'): 0.003003289854085004,\n",
       " (349, 'train'): 0.0023823074168629116,\n",
       " (349, 'val'): 0.0030037953897758766,\n",
       " (350, 'train'): 0.0023789016047009717,\n",
       " (350, 'val'): 0.0030049607157707214,\n",
       " (351, 'train'): 0.0023944682821079536,\n",
       " (351, 'val'): 0.00300524080241168,\n",
       " (352, 'train'): 0.002374319980541865,\n",
       " (352, 'val'): 0.003004302305203897,\n",
       " (353, 'train'): 0.0024005685139585425,\n",
       " (353, 'val'): 0.0030057377837322376,\n",
       " (354, 'train'): 0.0023391677963512914,\n",
       " (354, 'val'): 0.003003089792198605,\n",
       " (355, 'train'): 0.0023836344480514526,\n",
       " (355, 'val'): 0.0030054278947688915,\n",
       " (356, 'train'): 0.0023497961185596607,\n",
       " (356, 'val'): 0.003005770069581491,\n",
       " (357, 'train'): 0.0023508978386720023,\n",
       " (357, 'val'): 0.003004909665496261,\n",
       " (358, 'train'): 0.0024023145023319456,\n",
       " (358, 'val'): 0.0030059303950380396,\n",
       " (359, 'train'): 0.002390609985148465,\n",
       " (359, 'val'): 0.0030053379359068692,\n",
       " (360, 'train'): 0.002364728875734188,\n",
       " (360, 'val'): 0.0030049292577637565,\n",
       " (361, 'train'): 0.002357764169573784,\n",
       " (361, 'val'): 0.0030036400313730592,\n",
       " (362, 'train'): 0.002373316980622433,\n",
       " (362, 'val'): 0.0030040125604029054,\n",
       " (363, 'train'): 0.002380901809643816,\n",
       " (363, 'val'): 0.0030039482646518284,\n",
       " (364, 'train'): 0.002380404759336401,\n",
       " (364, 'val'): 0.0030045503819430314,\n",
       " (365, 'train'): 0.00238889221239973,\n",
       " (365, 'val'): 0.0030043519757412098,\n",
       " (366, 'train'): 0.0023843181767949353,\n",
       " (366, 'val'): 0.0030042173133956063,\n",
       " (367, 'train'): 0.002389236111883764,\n",
       " (367, 'val'): 0.003004888141596759,\n",
       " (368, 'train'): 0.002385820503588076,\n",
       " (368, 'val'): 0.0030048886934916177,\n",
       " (369, 'train'): 0.0023951131022638744,\n",
       " (369, 'val'): 0.0030057215028338963,\n",
       " (370, 'train'): 0.002364098404844602,\n",
       " (370, 'val'): 0.003005780279636383,\n",
       " (371, 'train'): 0.002376277827554279,\n",
       " (371, 'val'): 0.0030045023670902957,\n",
       " (372, 'train'): 0.002374902229618143,\n",
       " (372, 'val'): 0.003003942193808379,\n",
       " (373, 'train'): 0.0023724661657103787,\n",
       " (373, 'val'): 0.0030023510809297914,\n",
       " (374, 'train'): 0.002379929370902203,\n",
       " (374, 'val'): 0.0030037920784067224,\n",
       " (375, 'train'): 0.0023747327978964204,\n",
       " (375, 'val'): 0.003002201517422994,\n",
       " (376, 'train'): 0.0024101499606061865,\n",
       " (376, 'val'): 0.003003783799983837,\n",
       " (377, 'train'): 0.002384557906124327,\n",
       " (377, 'val'): 0.0030044096487539785,\n",
       " (378, 'train'): 0.0023956576845160235,\n",
       " (378, 'val'): 0.003003529100506394,\n",
       " (379, 'train'): 0.0023867729361410493,\n",
       " (379, 'val'): 0.003004218969080183,\n",
       " (380, 'train'): 0.0023842211812734604,\n",
       " (380, 'val'): 0.0030059883439982377,\n",
       " (381, 'train'): 0.002355842195727207,\n",
       " (381, 'val'): 0.0030040740966796875,\n",
       " (382, 'train'): 0.0023878160174246187,\n",
       " (382, 'val'): 0.0030045605919979236,\n",
       " (383, 'train'): 0.0023592634609452,\n",
       " (383, 'val'): 0.0030040986560009144,\n",
       " (384, 'train'): 0.0024163218008147348,\n",
       " (384, 'val'): 0.003005433689664911,\n",
       " (385, 'train'): 0.0023699521466537758,\n",
       " (385, 'val'): 0.0030059916553673923,\n",
       " (386, 'train'): 0.0023610625002119276,\n",
       " (386, 'val'): 0.003005154706813671,\n",
       " (387, 'train'): 0.002358740195631981,\n",
       " (387, 'val'): 0.00300379345814387,\n",
       " (388, 'train'): 0.0023778618347865565,\n",
       " (388, 'val'): 0.0030043859172750403,\n",
       " (389, 'train'): 0.0024217311292886734,\n",
       " (389, 'val'): 0.0030066191598221107,\n",
       " (390, 'train'): 0.0023499213297058035,\n",
       " (390, 'val'): 0.0030061345961358813,\n",
       " (391, 'train'): 0.002361316578807654,\n",
       " (391, 'val'): 0.0030048056333153335,\n",
       " (392, 'train'): 0.0023574993980151637,\n",
       " (392, 'val'): 0.0030026032968803688,\n",
       " (393, 'train'): 0.0023620984068623294,\n",
       " (393, 'val'): 0.0030003038269502146,\n",
       " (394, 'train'): 0.002397713561852773,\n",
       " (394, 'val'): 0.0030021510190433925,\n",
       " (395, 'train'): 0.0023753459530848043,\n",
       " (395, 'val'): 0.0030003002396336307,\n",
       " (396, 'train'): 0.002377549186348915,\n",
       " (396, 'val'): 0.003002547831447036,\n",
       " (397, 'train'): 0.0023453281157546574,\n",
       " (397, 'val'): 0.0030013733991870175,\n",
       " (398, 'train'): 0.0023814062415449706,\n",
       " (398, 'val'): 0.003003309722299929,\n",
       " (399, 'train'): 0.0023825710156449567,\n",
       " (399, 'val'): 0.003003687494330936,\n",
       " (400, 'train'): 0.0023547722095692597,\n",
       " (400, 'val'): 0.0030021283913541723,\n",
       " (401, 'train'): 0.002368040865770093,\n",
       " (401, 'val'): 0.0030010701329619797,\n",
       " (402, 'train'): 0.0023513109319739873,\n",
       " (402, 'val'): 0.0030014928844239977,\n",
       " (403, 'train'): 0.0023591643958180038,\n",
       " (403, 'val'): 0.0030032779883455347,\n",
       " (404, 'train'): 0.002363500702712271,\n",
       " (404, 'val'): 0.00300301859776179,\n",
       " (405, 'train'): 0.0023729142353490548,\n",
       " (405, 'val'): 0.0030034090633745546,\n",
       " (406, 'train'): 0.0023645419903375485,\n",
       " (406, 'val'): 0.0030025332062332717,\n",
       " (407, 'train'): 0.0023775711241695615,\n",
       " (407, 'val'): 0.00300284778630292,\n",
       " (408, 'train'): 0.002419878832168049,\n",
       " (408, 'val'): 0.0030054171328191405,\n",
       " (409, 'train'): 0.0023828479288904754,\n",
       " (409, 'val'): 0.0030033431119389003,\n",
       " (410, 'train'): 0.0023676911713900388,\n",
       " (410, 'val'): 0.0030038770702150133,\n",
       " (411, 'train'): 0.002347847377812421,\n",
       " (411, 'val'): 0.0030044513168158353,\n",
       " (412, 'train'): 0.0023401883879193555,\n",
       " (412, 'val'): 0.0030027122961150277,\n",
       " (413, 'train'): 0.0023484579804870817,\n",
       " (413, 'val'): 0.003002054989337921,\n",
       " (414, 'train'): 0.002384508166600157,\n",
       " (414, 'val'): 0.0030027892854478625,\n",
       " (415, 'train'): 0.0023559903794968568,\n",
       " (415, 'val'): 0.0030036736969594603,\n",
       " (416, 'train'): 0.002411435944614587,\n",
       " (416, 'val'): 0.0030042120703944455,\n",
       " (417, 'train'): 0.0023594993270105785,\n",
       " (417, 'val'): 0.003004048157621313,\n",
       " (418, 'train'): 0.002339661949210697,\n",
       " (418, 'val'): 0.003004162123909703,\n",
       " (419, 'train'): 0.002363564791502776,\n",
       " (419, 'val'): 0.0030043784666944433,\n",
       " (420, 'train'): 0.0023326261865871923,\n",
       " (420, 'val'): 0.0030027351997516773,\n",
       " (421, 'train'): 0.002365710903648977,\n",
       " (421, 'val'): 0.0030039675809718945,\n",
       " (422, 'train'): 0.0023527145385742188,\n",
       " (422, 'val'): 0.003001354082866951,\n",
       " (423, 'train'): 0.00235888893129649,\n",
       " (423, 'val'): 0.0030006222702838757,\n",
       " (424, 'train'): 0.002365973122693874,\n",
       " (424, 'val'): 0.003000734304940259,\n",
       " (425, 'train'): 0.0023551734371317756,\n",
       " (425, 'val'): 0.0030014605985747445,\n",
       " (426, 'train'): 0.0023771028413816734,\n",
       " (426, 'val'): 0.0030021397051987826,\n",
       " (427, 'train'): 0.002403126546630153,\n",
       " (427, 'val'): 0.003004288507832421,\n",
       " (428, 'train'): 0.0023658745404746798,\n",
       " (428, 'val'): 0.0030048718606984176,\n",
       " (429, 'train'): 0.002358029286066691,\n",
       " (429, 'val'): 0.003003669833695447,\n",
       " (430, 'train'): 0.0024219740320135046,\n",
       " (430, 'val'): 0.0030046381332256176,\n",
       " (431, 'train'): 0.0023756197619217412,\n",
       " (431, 'val'): 0.0030055156460514773,\n",
       " (432, 'train'): 0.002328811006413566,\n",
       " (432, 'val'): 0.003002514165860635,\n",
       " (433, 'train'): 0.002361433580517769,\n",
       " (433, 'val'): 0.0030018416819749057,\n",
       " (434, 'train'): 0.0023680950894399925,\n",
       " (434, 'val'): 0.003002319346975397,\n",
       " (435, 'train'): 0.002350022050517577,\n",
       " (435, 'val'): 0.0030026496560485276,\n",
       " (436, 'train'): 0.002389215967721409,\n",
       " (436, 'val'): 0.003003480809706229,\n",
       " (437, 'train'): 0.0023712805575794643,\n",
       " (437, 'val'): 0.003002869862097281,\n",
       " (438, 'train'): 0.0023516472429037094,\n",
       " (438, 'val'): 0.003004282161041542,\n",
       " (439, 'train'): 0.0023854060305489432,\n",
       " (439, 'val'): 0.003004869653118981,\n",
       " (440, 'train'): 0.0023813313908047145,\n",
       " (440, 'val'): 0.003004922635025448,\n",
       " (441, 'train'): 0.0023512987213002313,\n",
       " (441, 'val'): 0.003004442486498091,\n",
       " (442, 'train'): 0.002380069621183254,\n",
       " (442, 'val'): 0.0030048081168421994,\n",
       " (443, 'train'): 0.0024167479326327643,\n",
       " (443, 'val'): 0.0030065675576527915,\n",
       " (444, 'train'): 0.002361994305694545,\n",
       " (444, 'val'): 0.0030048608228012367,\n",
       " (445, 'train'): 0.0024102170848184163,\n",
       " (445, 'val'): 0.0030052995792141666,\n",
       " (446, 'train'): 0.0023673315429025227,\n",
       " (446, 'val'): 0.0030033226918291162,\n",
       " (447, 'train'): 0.0023598699933952754,\n",
       " (447, 'val'): 0.0030020889308717516,\n",
       " (448, 'train'): 0.0023999648789564767,\n",
       " (448, 'val'): 0.0030017870443838613,\n",
       " (449, 'train'): 0.0023730388946003383,\n",
       " (449, 'val'): 0.0030033232437239755,\n",
       " (450, 'train'): 0.002353602330441828,\n",
       " (450, 'val'): 0.0030028475103554904,\n",
       " (451, 'train'): 0.002349944509289883,\n",
       " (451, 'val'): 0.0030027487211757237,\n",
       " (452, 'train'): 0.0023767261731403844,\n",
       " (452, 'val'): 0.0030041110736352427,\n",
       " (453, 'train'): 0.0023811196011525614,\n",
       " (453, 'val'): 0.003003954059547848,\n",
       " (454, 'train'): 0.002329441132368865,\n",
       " (454, 'val'): 0.003001618164556998,\n",
       " (455, 'train'): 0.0023510712026445953,\n",
       " (455, 'val'): 0.0030018800386676084,\n",
       " (456, 'train'): 0.002368707485772945,\n",
       " (456, 'val'): 0.003003395541950508,\n",
       " (457, 'train'): 0.002367362242054056,\n",
       " (457, 'val'): 0.0030057107408841452,\n",
       " (458, 'train'): 0.0023886592437823615,\n",
       " (458, 'val'): 0.0030040696815208153,\n",
       " (459, 'train'): 0.002378657736160137,\n",
       " (459, 'val'): 0.0030050813047974197,\n",
       " (460, 'train'): 0.0023480023912809513,\n",
       " (460, 'val'): 0.003004076580206553,\n",
       " (461, 'train'): 0.002350905841147458,\n",
       " (461, 'val'): 0.003004170126385159,\n",
       " (462, 'train'): 0.00236565212684649,\n",
       " (462, 'val'): 0.0030039043890105355,\n",
       " (463, 'train'): 0.002377883634633488,\n",
       " (463, 'val'): 0.0030040426386727225,\n",
       " (464, 'train'): 0.002369905580525045,\n",
       " (464, 'val'): 0.0030028483381977786,\n",
       " (465, 'train'): 0.0023729229966799417,\n",
       " (465, 'val'): 0.003002445454950686,\n",
       " (466, 'train'): 0.002383709091831137,\n",
       " (466, 'val'): 0.0030016565212497006,\n",
       " (467, 'train'): 0.002398199988184152,\n",
       " (467, 'val'): 0.0030032004471178407,\n",
       " (468, 'train'): 0.0023731790758945324,\n",
       " (468, 'val'): 0.003003966753129606,\n",
       " (469, 'train'): 0.0023835664959969342,\n",
       " (469, 'val'): 0.0030055145422617593,\n",
       " (470, 'train'): 0.0023929693356708245,\n",
       " (470, 'val'): 0.003005584356961427,\n",
       " (471, 'train'): 0.0023770851807461846,\n",
       " (471, 'val'): 0.00300476782851749,\n",
       " (472, 'train'): 0.0023749109909490303,\n",
       " (472, 'val'): 0.0030050125938874705,\n",
       " (473, 'train'): 0.0023934832877582973,\n",
       " (473, 'val'): 0.003006192821043509,\n",
       " (474, 'train'): 0.0023755734717404403,\n",
       " (474, 'val'): 0.003005963784677011,\n",
       " (475, 'train'): 0.0023923636310630375,\n",
       " (475, 'val'): 0.0030058506462309095,\n",
       " (476, 'train'): 0.0023488544479564385,\n",
       " (476, 'val'): 0.0030044182031242934,\n",
       " (477, 'train'): 0.00238965865638521,\n",
       " (477, 'val'): 0.003003261155552334,\n",
       " (478, 'train'): 0.0023616867622843494,\n",
       " (478, 'val'): 0.00300383347052115,\n",
       " (479, 'train'): 0.002367193017292906,\n",
       " (479, 'val'): 0.003004435035917494,\n",
       " (480, 'train'): 0.002371633977249817,\n",
       " (480, 'val'): 0.0030037998049347488,\n",
       " (481, 'train'): 0.0023563684274752936,\n",
       " (481, 'val'): 0.0030021134901929785,\n",
       " (482, 'train'): 0.002394395362999704,\n",
       " (482, 'val'): 0.003003204862276713,\n",
       " (483, 'train'): 0.0023660783966382346,\n",
       " (483, 'val'): 0.003003302271719332,\n",
       " (484, 'train'): 0.002356391469085658,\n",
       " (484, 'val'): 0.0030008013601656312,\n",
       " (485, 'train'): 0.0023223762572915467,\n",
       " (485, 'val'): 0.0030000990739575137,\n",
       " (486, 'train'): 0.0023684455426754773,\n",
       " (486, 'val'): 0.003001677217306914,\n",
       " (487, 'train'): 0.0023458993269337546,\n",
       " (487, 'val'): 0.0030010229459515323,\n",
       " (488, 'train'): 0.002338882949617174,\n",
       " (488, 'val'): 0.00300054003794988,\n",
       " (489, 'train'): 0.002371960699006363,\n",
       " (489, 'val'): 0.003000498921782882,\n",
       " (490, 'train'): 0.002377760355119352,\n",
       " (490, 'val'): 0.003000140742019371,\n",
       " (491, 'train'): 0.0023755705053055726,\n",
       " (491, 'val'): 0.0030017426168477096,\n",
       " (492, 'train'): 0.002374296800957786,\n",
       " (492, 'val'): 0.003002395232518514,\n",
       " (493, 'train'): 0.002370868292119768,\n",
       " (493, 'val'): 0.0030035434497727286,\n",
       " (494, 'train'): 0.002374907886540448,\n",
       " (494, 'val'): 0.0030025483833418954,\n",
       " (495, 'train'): 0.002379672877766468,\n",
       " (495, 'val'): 0.0030031847181143582,\n",
       " (496, 'train'): 0.0023791919703836793,\n",
       " (496, 'val'): 0.003003317172880526,\n",
       " (497, 'train'): 0.0023666580242139323,\n",
       " (497, 'val'): 0.0030035219258732265,\n",
       " (498, 'train'): 0.002382146332550932,\n",
       " (498, 'val'): 0.0030052245215133385,\n",
       " (499, 'train'): 0.0023797596632330505,\n",
       " (499, 'val'): 0.0030058183603816563}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train autoencoder\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 不需要label，所以用一个占位符\"_\"代替\n",
    "    for batchidx, (x, _) in enumerate(X_allDataLoader):\n",
    "        x.requires_grad_(True)\n",
    "        # encode and decode \n",
    "        output = model(x)\n",
    "        # compute loss\n",
    "        print(output.shape)\n",
    "        loss = loss_function(output, x)      \n",
    "        # update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "           \n",
    "    loss_train[epoch,0] = loss.item()  \n",
    "    print('Epoch: %04d, Training loss=%.8f' %\n",
    "          (epoch+1, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'saved/models/ae.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_batch = model(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(recon_batch,trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = model.encode(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = train_embeddings.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.Lasso(alpha=0.001)\n",
    "clf.fit(feature, Y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RFrg = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "RFrg.fit(feature, Y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFeature = model.encode(testData)\n",
    "lasso = clf.predict(testFeature.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfresult = RFrg.predict(testFeature.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(lasso,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(rfresult,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.scatter(tsne_results[:, 0], tsne_results[:, 1], label=\"label\")\n",
    "plt.legend()\n",
    "plt.savefig(\"saved/figures/tsne_vae_gdsc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# data type conversion\n",
    "B_feature = torch.FloatTensor(feature).to(device)\n",
    "y = torch.FloatTensor(Y_train.values).to(device)\n",
    "# construct TensorDataset\n",
    "b_data = TensorDataset(B_feature, y)\n",
    "trainDataLoader2 = DataLoader(dataset=b_data, batch_size=200, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization DNN model\n",
    "\n",
    "predictor = DNN(128, dim_dnn_out).to(device)\n",
    "optimizer = optim.Adam(predictor.parameters(), lr=1e-3,betas=(0.9,0.99))\n",
    "#loss1-softmax\n",
    "loss_func = nn.MSELoss().to(device)\n",
    "#loss2-sigmoid\n",
    "#loss_func = nn.BCELoss()\n",
    "#loss3-sigmoid\n",
    "#loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "#criterion = torch.nn.MSELoss(size_average=True)\n",
    "#criterion = torch.nn.BCELoss(size_average=True) # Defined loss function\n",
    "#optimizer = optim.Adm(model.parameters(), lr=0.01) # Defined optimizer\n",
    "loss_train = np.zeros((epochs, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "for epoch in range(EPOCH):\n",
    "    print('Epoch: ',epoch)\n",
    "    for step,(batch_x,batch_y) in enumerate(trainDataLoader2):\n",
    "        b_x = Variable(batch_x)\n",
    "        b_y = Variable(batch_y)\n",
    "        # predict label\n",
    "        output = predictor(b_x)\n",
    "        # b_y=F.sigmoid(b_y) \n",
    "        \n",
    "        #print\n",
    "        #print(output)\n",
    "        #print(b_y)\n",
    "        # compute loss\n",
    "        loss = loss_func(output,b_y)\n",
    "        #loss = criterion(output, b_y)\n",
    "        \n",
    "        # update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_train[epoch,0] = loss.item()  \n",
    "    print('Epoch: %04d, Training loss=%.8f' %\n",
    "          (epoch+1, loss.item())) \n",
    "\n",
    "# Save model\n",
    "torch.save(predictor.state_dict(), 'saved/models/DNN_GDSC.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpredict = predictor(testFeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(testpredict.detach().cpu().numpy(),Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(testpredict.detach().cpu().numpy(),Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
